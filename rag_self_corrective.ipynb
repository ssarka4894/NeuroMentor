{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713a56a3-525c-4033-abeb-85e1456dde76",
   "metadata": {},
   "source": [
    "## Check the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de710bda-3c8d-44f1-aced-678b1a73e007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/osave/1402SJL-ai-accelerated-spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ab1cf-fcaf-4344-937c-c9e43bf0a695",
   "metadata": {},
   "source": [
    "## Enhanced Document Loading System\n",
    "\n",
    "This cell implements an enhanced document loading system that combines web content with local files to create a comprehensive knowledge base. The system loads documents from multiple sources and adds appropriate metadata for citation tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Loads URLs from CSV file with recursive depth\n",
    "- Supports both Jupyter notebooks (.ipynb) and Python files (.py)\n",
    "- Adds source type metadata for proper citation handling\n",
    "- Provides detailed loading progress and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d4cdd2-630d-4d5e-a150-6e435c523333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED DOCUMENT LOADING\n",
      "============================================================\n",
      "Found 108 URLs to load\n",
      "‚úì Loaded: https://github.com/rapidsai/cuml\n",
      "‚úì Loaded: https://docs.rapids.ai/api/cuml/stable/\n",
      "‚úì Loaded: https://github.com/rapidsai/cudf\n",
      "‚úì Loaded: https://www.exxactcorp.com/blog/Deep-Learning/nvidia-rapids-tutorial\n",
      "‚úì Loaded: https://colab.google/articles/cudf\n",
      "‚úì Loaded: https://developer.nvidia.com/blog/nvidia-cuml-brings-zero-code-change-acceleration-to-scikit-learn/\n",
      "‚úì Loaded: https://stackoverflow.com/questions/60188071/installing-cudf-cuml-into-colab-with-rapids-ai-version-0-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai25.07/lib/python3.12/site-packages/langchain_community/document_loaders/recursive_url_loader.py:43: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(raw_html, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded: https://inside-machinelearning.com/en/cuml-tutorial-colab/\n",
      "‚úì Loaded: https://www.analyticsvidhya.com/blog/2022/01/cuml-blazing-fast-machine-learning-model-training-with-nvidias-rapids/\n",
      "‚úì Loaded: https://rapids.ai/\n",
      "‚úì Loaded: https://github.com/rapidsai/cugraph\n",
      "‚úì Loaded: https://docs.rapids.ai/api/cugraph/stable/\n",
      "‚úì Loaded: https://github.com/rapidsai/cugraph/issues/1200\n",
      "‚úì Loaded: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/cugraph/gat_conv.html\n",
      "‚úì Loaded: https://github.com/rapidsai/cuspatial\n",
      "‚úì Loaded: https://docs.rapids.ai/api/cuspatial/stable/user_guide/cuspatial_api_examples/\n",
      "‚úì Loaded: https://docs.rapids.ai/user-guide/\n",
      "‚úì Loaded: https://docs.rapids.ai/api/cuspatial/stable/\n",
      "‚úì Loaded: https://pypi.org/project/cuspatial-cu11/\n",
      "‚úì Loaded: https://learning.rc.virginia.edu/notes/rapids/\n",
      "‚úì Loaded: https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
      "‚úì Loaded: https://docs.pytorch.org/tutorials/\n",
      "‚úì Loaded: https://docs.pytorch.org/tutorials/beginner/basics/intro.html\n",
      "‚úì Loaded: https://github.com/facebookresearch/PyTorch-BigGraph\n",
      "‚úì Loaded: https://www.geeksforgeeks.org/deep-learning/pytorch-learn-with-examples/\n",
      "‚úì Loaded: https://www.nvidia.com/en-sg/data-center/gpu-accelerated-applications/tensorflow/\n",
      "‚úì Loaded: https://www.tensorflow.org/guide/gpu\n",
      "‚úì Loaded: https://gist.github.com/j-min/baae1aa56e861cab9831b3722755ae6d\n",
      "‚úì Loaded: https://stackoverflow.com/questions/51306862/how-do-i-use-tensorflow-gpu\n",
      "‚úì Loaded: https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/\n",
      "‚úì Loaded: https://neptune.ai/blog/installing-tensorflow-2-gpu-guide\n",
      "‚úì Loaded: https://www.databricks.com/tensorflow/using-a-gpu\n",
      "‚úì Loaded: https://www.kaggle.com/code/hassanamin/tensorflow-mnist-gpu-tutorial\n",
      "‚úì Loaded: https://acecloud.ai/resources/blog/tensorflow-gpu/\n",
      "‚úì Loaded: https://www.tensorflow.org/install/pip\n",
      "‚úì Loaded: https://github.com/rapidsai/cuxfilter\n",
      "‚úì Loaded: https://rapidsai.github.io/cuxfilter/10 minutes to cuxfilter.html\n",
      "‚úì Loaded: https://docs.rapids.ai/api/cuxfilter/stable/\n",
      "‚úì Loaded: https://anaconda.org/rapidsai-nightly/cuxfilter\n",
      "‚úì Loaded: https://github.com/rapidsai-community/showcase/blob/main/team_contributions/cuxfilter-tutorial/cuxfilter_tutorial.ipynb\n",
      "‚úì Loaded: https://documen.tician.de/pycuda/\n",
      "‚úì Loaded: https://github.com/minrk/PyCUDA/blob/master/examples/demo.py\n",
      "‚úì Loaded: https://huggingface.co/docs/transformers/index\n",
      "‚úì Loaded: https://github.com/huggingface/transformers\n",
      "‚úì Loaded: https://huggingface.co/docs/hub/transformers\n",
      "‚úì Loaded: https://github.com/huggingface/transformers/issues/2704\n",
      "‚úì Loaded: https://realpython.com/huggingface-transformers/\n",
      "‚úì Loaded: https://mljourney.com/getting-started-with-hugging-face-transformers/\n",
      "‚úì Loaded: https://huggingface.co/docs/transformers/en/quicktour\n",
      "‚úì Loaded: https://www.kdnuggets.com/using-hugging-face-transformers-with-pytorch-and-tensorflow\n",
      "‚úì Loaded: https://huggingface.co/docs/transformers/perf_infer_gpu_one\n",
      "‚úì Loaded: https://huggingface.co/docs/transformers/en/installation\n",
      "‚úì Loaded: https://github.com/rkinas/cuda-learning\n",
      "‚úì Loaded: https://github.com/NVIDIA/DeepLearningExamples\n",
      "‚úì Loaded: https://github.com/topics/cuda\n",
      "‚úì Loaded: https://github.com/PacktPublishing/Learn-CUDA-Programming\n",
      "‚úì Loaded: https://github.com/topics/cuda-programming?l=cuda\n",
      "‚úì Loaded: https://cuda-tutorial.github.io/\n",
      "‚úì Loaded: https://vistalab-technion.github.io/cs236781/semesters/w1920/tutorials/tutorial_10/\n",
      "‚úì Loaded: https://github.com/puttsk/cuda-tutorial\n",
      "‚úì Loaded: https://github.com/NVIDIA/cuda-samples\n",
      "‚úì Loaded: https://developer.nvidia.com/blog/nlp-and-text-precessing-with-rapids-now-simpler-and-faster/\n",
      "‚úì Loaded: https://www.researchgate.net/publication/305637400_Performance_Evaluation_and_Benchmarking_of_Modern_GPU_Architectures\n",
      "‚úì Loaded: https://www.researchgate.net/publication/347824334_A_Taste_of_Scientific_Computing_on_the_GPU-Accelerated_Edge_Device\n",
      "‚úì Loaded: https://www.researchgate.net/publication/46390957_GPU_Accelerated_Scientific_Computing_Evaluation_of_the_NVIDIA_Fermi_Architecture_Elementary_Kernels_and_Linear_Solvers\n",
      "‚úì Loaded: https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n",
      "‚úì Loaded: https://www.stat.berkeley.edu/~scf/paciorek-gpuWorkshop.html\n",
      "‚úì Loaded: https://docs.nvidia.com/deeplearning/performance/index.html\n",
      "‚úì Loaded: https://developer.nvidia.com/deep-learning-frameworks\n",
      "‚úì Loaded: https://docs.nvidia.com/deeplearning/nemo/best-practices/index.html\n",
      "‚úì Loaded: https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html\n",
      "‚úì Loaded: https://developer.nvidia.com/deep-learning\n",
      "‚úì Loaded: https://docs.nvidia.com/deeplearning/performance/recsys-best-practices/index.html\n",
      "‚úì Loaded: https://docs.nvidia.com/deeplearning/frameworks/index.html\n",
      "‚úì Loaded: https://developer.nvidia.com/deep-learning-examples\n",
      "‚úì Loaded: https://research.nvidia.com/publications\n",
      "‚úì Loaded: https://www.nvidia.com/en-us/glossary/xgboost/\n",
      "‚úì Loaded: https://xgboost.readthedocs.io/en/stable/gpu/index.html\n",
      "‚úì Loaded: https://www.numberanalytics.com/blog/optimizing-ml-with-xgboost-strategies\n",
      "‚úì Loaded: https://www.kaggle.com/couyang/a-guide-of-xgboost-with-gpu\n",
      "‚úì Loaded: https://www.gpu-mart.com/gpu-xgboost\n",
      "‚úì Loaded: https://developer.ibm.com/tutorials/awb-implement-xgboost-in-python/\n",
      "‚úì Loaded: https://www.datacamp.com/tutorial/xgboost-in-python\n",
      "‚úì Loaded: https://xgboost.readthedocs.io/en/release_0.81/gpu/index.html\n",
      "‚úì Loaded: https://github.com/Azure/fast_retraining/issues/62\n",
      "‚úì Loaded: https://numba.pydata.org/numba-doc/0.13/CUDAJit.html\n",
      "‚úì Loaded: https://nyu-cds.github.io/python-numba/05-cuda/\n",
      "‚úì Loaded: https://stackoverflow.com/questions/78162405/how-to-use-numba-cuda-jit-decorator\n",
      "‚úì Loaded: https://numba.pydata.org/numba-doc/dev/cuda/index.html\n",
      "‚úì Loaded: https://www.kaggle.com/code/harshwalia/1-introduction-to-cuda-python-with-numba\n",
      "‚úì Loaded: https://numba.readthedocs.io/en/stable/user/5minguide.html\n",
      "‚úì Loaded: https://numba.readthedocs.io/en/stable/cuda/examples.html\n",
      "‚úì Loaded: https://raw.githubusercontent.com/numba/nvidia-cuda-tutorial/main/numba-for-cuda-programmers-complete.pdf\n",
      "‚úì Loaded: https://cupy.dev/\n",
      "‚úì Loaded: https://docs.cupy.dev/en/stable/\n",
      "‚úì Loaded: https://github.com/cupy/cupy\n",
      "‚úì Loaded: https://docs.cupy.dev/en/stable/user_guide/basic.html\n",
      "‚úì Loaded: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
      "‚úì Loaded: https://github.com/google/jax\n",
      "‚úì Loaded: https://github.com/NVIDIA/accelerated-computing-hub/tree/main\n",
      "‚úì Loaded: https://developer.nvidia.com/blog/category/data-science/\n",
      "‚úì Loaded: https://developer.nvidia.com/blog/rapids-brings-zero-code-change-acceleration-io-performance-gains-and-out-of-core-xgboost/\n",
      "‚úì Loaded: https://developer.nvidia.com/blog/ai-in-manufacturing-and-operations-at-nvidia-accelerating-ml-models-with-nvidia-cuda-x-data-science/\n",
      "‚úì Loaded: https://medium.com/rapids-ai\n",
      "‚úì Loaded: https://medium.com/rapids-ai/rapids-23-08-release-23db51c255f0\n",
      "‚úì Loaded: https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d\n",
      "‚úì Loaded: https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\n",
      "‚úì Loaded: https://www.unum.cloud/blog/2022-01-26-cupy\n",
      "URL documents loaded: 1208\n",
      "\n",
      "Loading files from: resource/\n",
      "Found 48 notebook files and 21 Python files\n",
      "‚úì Loaded: Pagerank.ipynb\n",
      "‚úì Loaded: 5.3_Cupy_nvmath.ipynb\n",
      "‚úì Loaded: 2.0_Numba.ipynb\n",
      "‚úì Loaded: bfs_benchmark.ipynb\n",
      "‚úì Loaded: synth_release_single_node_multi_gpu.ipynb\n",
      "‚úì Loaded: 2.1_Numba_lab.ipynb\n",
      "‚úì Loaded: CostMatrix.ipynb\n",
      "‚úì Loaded: Chapter_04_Scientific_Computing_with_CuPy.ipynb\n",
      "‚úì Loaded: Chapter_06_Intro_to_nvmath-python.ipynb\n",
      "‚úì Loaded: 7.0_cuML.ipynb\n",
      "‚úì Loaded: 4.1_CUDA_Array_Interface.ipynb\n",
      "‚úì Loaded: 3.1_Numba_lab_2.ipynb\n",
      "‚úì Loaded: synth_release_single_gpu.ipynb\n",
      "‚úì Loaded: release.ipynb\n",
      "‚úì Loaded: Chapter_02_Brief_Intro_to_CUDA.ipynb\n",
      "‚úì Loaded: Chapter_01_GPU_Computing_Basics.ipynb\n",
      "‚úì Loaded: 3.0_Numba_gauss.ipynb\n",
      "‚úì Loaded: 8.0_Multi-GPU_with_Dask.ipynb\n",
      "‚úì Loaded: umap_demo_full.ipynb\n",
      "‚úì Loaded: 2.2_Numba_lab_solution.ipynb\n",
      "‚úì Loaded: line_intersection.ipynb\n",
      "‚úì Loaded: Chapter_10_Developer_Tools.ipynb\n",
      "‚úì Loaded: nx_cugraph_bc_benchmarking.ipynb\n",
      "‚úì Loaded: 5.0_Cupy.ipynb\n",
      "‚úì Loaded: Chapter_05_CUDA_Kernels_with_Numba.ipynb\n",
      "‚úì Loaded: Chapter_09_Intro_to_cuGraph.ipynb\n",
      "‚úì Loaded: random_walk_perf.ipynb\n",
      "‚úì Loaded: ai_with_a_conscience.ipynb\n",
      "‚úì Loaded: 5.1_Cupy_Lab.ipynb\n",
      "‚úì Loaded: sssp_benchmark.ipynb\n",
      "‚úì Loaded: 3.2_Numba_lab_2_solution.ipynb\n",
      "‚úì Loaded: Chapter_03_Python_on_the_GPU.ipynb\n",
      "‚úì Loaded: NYCTaxi-E2E.ipynb\n",
      "‚úì Loaded: 4.0_pyNVML.ipynb\n",
      "‚úì Loaded: weather.ipynb\n",
      "‚úì Loaded: census_education2income_demo.ipynb\n",
      "‚úì Loaded: Chapter_07_Intro_to_cuDF.ipynb\n",
      "‚úì Loaded: 1.0_CPU_GPU_Comparison.ipynb\n",
      "‚úì Loaded: 0.0_Welcome.ipynb\n",
      "‚úì Loaded: random_walk_benchmark.ipynb\n",
      "‚úì Loaded: gen_550M.ipynb\n",
      "‚úì Loaded: 5.2_Cupy_Lab_solution.ipynb\n",
      "‚úì Loaded: Symmetrize.ipynb\n",
      "‚úì Loaded: Renumber-2.ipynb\n",
      "‚úì Loaded: Chapter_11_Distributed_Computing_cuPyNumeric.ipynb\n",
      "‚úì Loaded: Chapter_08_Intro_to_cuML.ipynb\n",
      "‚úì Loaded: Renumber.ipynb\n",
      "‚úì Loaded: 6.0_cuDF.ipynb\n",
      "‚úì Loaded: libdevicefuncs.py\n",
      "‚úì Loaded: simulator_init.py\n",
      "‚úì Loaded: decorators.py\n",
      "‚úì Loaded: cudaimpl.py\n",
      "‚úì Loaded: compiler.py\n",
      "‚úì Loaded: target.py\n",
      "‚úì Loaded: libdevice.py\n",
      "‚úì Loaded: cuda_paths.py\n",
      "‚úì Loaded: libdeviceimpl.py\n",
      "‚úì Loaded: cudamath.py\n",
      "‚úì Loaded: nvvmutils.py\n",
      "‚úì Loaded: fig_helpers.py\n",
      "‚úì Loaded: libdevicedecl.py\n",
      "‚úì Loaded: cudadecl.py\n",
      "‚úì Loaded: printimpl.py\n",
      "‚úì Loaded: stubs.py\n",
      "‚úì Loaded: models.py\n",
      "‚úì Loaded: descriptor.py\n",
      "‚úì Loaded: random.py\n",
      "‚úì Loaded: testing.py\n",
      "‚úì Loaded: mathimpl.py\n",
      "Notebook documents loaded: 48\n",
      "Python file documents loaded: 21\n",
      "\n",
      "Total documents loaded: 1277\n",
      "Step 1 complete - enhanced loading done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import glob\n",
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders import NotebookLoader, TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: ENHANCED DOCUMENT LOADING (REPLACES YOUR ORIGINAL LOADING)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENHANCED DOCUMENT LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read URLs from CSV file\n",
    "df = pd.read_csv('gpu_data_science_urls.txt')\n",
    "base_urls = df['url'].dropna().tolist()\n",
    "\n",
    "print(f\"Found {len(base_urls)} URLs to load\")\n",
    "\n",
    "# Step 1: Load all web pages with recursive depth 2\n",
    "documents = []\n",
    "for url in base_urls:\n",
    "    try:\n",
    "        loader = RecursiveUrlLoader(url=url, max_depth=2,extractor=bs4_extractor)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Add source type metadata for URL documents\n",
    "        for doc in docs:\n",
    "            doc.metadata['source_type'] = 'url'\n",
    "        \n",
    "        documents.extend(docs)\n",
    "        print(f\"‚úì Loaded: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed: {url} - {e}\")\n",
    "\n",
    "print(f\"URL documents loaded: {len(documents)}\")\n",
    "\n",
    "# Step 2: Load notebooks and Python files from folder\n",
    "notebook_folder_path = 'resource/'  # CHANGE THIS TO YOUR FOLDER PATH\n",
    "\n",
    "if os.path.exists(notebook_folder_path):\n",
    "    print(f\"\\nLoading files from: {notebook_folder_path}\")\n",
    "    \n",
    "    # Find all .ipynb and .py files in the folder\n",
    "    notebook_files = glob.glob(os.path.join(notebook_folder_path, \"**/*.ipynb\"), recursive=True)\n",
    "    python_files = glob.glob(os.path.join(notebook_folder_path, \"**/*.py\"), recursive=True)\n",
    "    all_files = notebook_files + python_files\n",
    "    \n",
    "    print(f\"Found {len(notebook_files)} notebook files and {len(python_files)} Python files\")\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # Choose loader based on file extension\n",
    "            if file_path.endswith('.ipynb'):\n",
    "                loader = NotebookLoader(\n",
    "                    path=file_path,\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=1000,\n",
    "                    remove_newline=True,\n",
    "                )\n",
    "            else:  # .py files\n",
    "                loader = TextLoader(file_path)\n",
    "            \n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add custom metadata for both notebooks and Python files\n",
    "            for doc in docs:\n",
    "                relative_path = os.path.relpath(file_path, notebook_folder_path)\n",
    "                \n",
    "                if file_path.endswith('.ipynb'):\n",
    "                    source_type = 'notebook'\n",
    "                else:\n",
    "                    source_type = 'python_file'\n",
    "                \n",
    "                doc.metadata.update({\n",
    "                    'source_type': source_type,\n",
    "                    'source': relative_path,  # This will be folder/filename\n",
    "                    'full_path': file_path,\n",
    "                    'filename': os.path.basename(file_path)\n",
    "                })\n",
    "            \n",
    "            documents.extend(docs)\n",
    "            print(f\"‚úì Loaded: {relative_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    notebook_count = len([d for d in documents if d.metadata.get('source_type') == 'notebook'])\n",
    "    python_count = len([d for d in documents if d.metadata.get('source_type') == 'python_file'])\n",
    "    print(f\"Notebook documents loaded: {notebook_count}\")\n",
    "    print(f\"Python file documents loaded: {python_count}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Files folder not found: {notebook_folder_path}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "print(\"Step 1 complete - enhanced loading done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c024ee-b160-4456-83a0-42bbd1ed3896",
   "metadata": {},
   "source": [
    "## Enhanced Document Chunking System\n",
    "\n",
    "This cell implements an intelligent document chunking system that cleans, filters, and optimizes text chunks for better vector store performance. The system includes content quality filtering and adaptive chunk sizing based on document characteristics.\n",
    "\n",
    "**Key Components:**\n",
    "- HTML content cleaning with BeautifulSoup\n",
    "- Quality filtering to remove navigation and boilerplate text\n",
    "- Adaptive chunk size optimization based on document length analysis\n",
    "- Technical content-aware text splitting with code block preservation\n",
    "- Batch processing for memory efficiency\n",
    "- Fallback mechanism for error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b56d61-271a-4dd8-a2f9-4577e40b6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED TEXT CHUNKING\n",
      "============================================================\n",
      "Starting enhanced document chunking...\n",
      "Original documents: 1277\n",
      "Step 1: Cleaning document content...\n",
      "  Cleaned 0/1277 documents...\n",
      "  Cleaned 100/1277 documents...\n",
      "  Cleaned 200/1277 documents...\n",
      "  Cleaned 300/1277 documents...\n",
      "  Cleaned 400/1277 documents...\n",
      "  Cleaned 500/1277 documents...\n",
      "  Cleaned 600/1277 documents...\n",
      "  Cleaned 700/1277 documents...\n",
      "  Cleaned 800/1277 documents...\n",
      "  Cleaned 900/1277 documents...\n",
      "  Cleaned 1000/1277 documents...\n",
      "  Cleaned 1100/1277 documents...\n",
      "  Cleaned 1200/1277 documents...\n",
      "  ‚úì Cleaned: 1209 documents\n",
      "  ‚úó Skipped: 68 low-quality documents\n",
      "Step 2: Optimizing chunk parameters...\n",
      "  Using chunk_size: 1500\n",
      "  Using chunk_overlap: 300\n",
      "Step 3: Creating chunks...\n",
      "  Processed 0/1209 documents...\n",
      "  Processed 500/1209 documents...\n",
      "  Processed 1000/1209 documents...\n",
      "Step 4: Final quality filtering...\n",
      "Final Results:\n",
      "  Original documents: N/A\n",
      "  Cleaned documents: 1209\n",
      "  Total chunks created: 23308\n",
      "  Quality chunks: 21312\n",
      "  Average chunk length: 1240 characters\n",
      "‚úì Enhanced document chunking complete!\n",
      "\n",
      "üìÑ Sample chunk preview:\n",
      "Content: GitHub - rapidsai/cuml: cuML - RAPIDS Machine Learning Library Skip to content You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your ses...\n",
      "Metadata: {'source': 'https://github.com/rapidsai/cuml', 'content_type': 'text/html; charset=utf-8', 'title': 'GitHub - rapidsai/cuml: cuML - RAPIDS Machine Learning Library', 'description': 'cuML - RAPIDS Machine Learning Library. Contribute to rapidsai/cuml development by creating an account on GitHub.', 'language': 'en', 'source_type': 'url'}\n",
      "\n",
      "‚úÖ Text chunking completed successfully!\n",
      "Ready for vector store creation with 21312 high-quality chunks\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "def clean_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean document content by removing HTML, excessive whitespace, and noise\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw document content\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned content\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Remove HTML tags using BeautifulSoup\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements completely\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace and special characters\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple whitespace to single space\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
    "        # # Just remove actual problematic characters\n",
    "        # text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', ' ', text)  # Control chars only\n",
    "        text = re.sub(r'(.)\\1{3,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Remove URLs (optional - might want to keep some)\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Clean up and strip\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Content cleaning failed: {e}\")\n",
    "        # Fallback: basic cleaning\n",
    "        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "def filter_quality_content(text: str, min_length: int = 50) -> bool:\n",
    "    \"\"\"\n",
    "    Filter out low-quality content chunks\n",
    "    \n",
    "    Args:\n",
    "        text (str): Content to evaluate\n",
    "        min_length (int): Minimum character length\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if content meets quality criteria\n",
    "    \"\"\"\n",
    "    if len(text) < min_length:\n",
    "        return False\n",
    "    \n",
    "    # Check for meaningful content (not just navigation/metadata)\n",
    "    low_quality_indicators = [\n",
    "        'cookie policy', 'privacy policy', 'terms of service',\n",
    "        'sign up', 'log in', 'subscribe', 'newsletter',\n",
    "        'all rights reserved', 'copyright', '¬© 20',\n",
    "        'navigation menu', 'breadcrumb', 'skip to',\n",
    "        'loading...', 'javascript required'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    low_quality_count = sum(1 for indicator in low_quality_indicators if indicator in text_lower)\n",
    "    \n",
    "    # If more than 20% is low-quality indicators, filter out\n",
    "    if low_quality_count > len(low_quality_indicators) * 0.2:\n",
    "        return False\n",
    "    \n",
    "    # Check for reasonable word count and variety\n",
    "    words = text.split()\n",
    "    if len(words) < 10:  # Too few words\n",
    "        return False\n",
    "    \n",
    "    unique_words = len(set(words))\n",
    "    if unique_words / len(words) < 0.3:  # Too repetitive\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def optimize_chunk_size_for_content(documents: List, base_chunk_size: int = 1500) -> int:\n",
    "    \"\"\"\n",
    "    Analyze content to determine optimal chunk size\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents\n",
    "        base_chunk_size: Starting chunk size\n",
    "        \n",
    "    Returns:\n",
    "        int: Optimized chunk size\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return base_chunk_size\n",
    "    \n",
    "    # Sample some documents to analyze\n",
    "    sample_size = min(100, len(documents))\n",
    "    sample_docs = documents[:sample_size]\n",
    "    \n",
    "    avg_doc_length = sum(len(doc.page_content) for doc in sample_docs) / sample_size\n",
    "    \n",
    "    # Adjust chunk size based on average document length\n",
    "    if avg_doc_length < 500:\n",
    "        return 800  # Smaller chunks for short docs\n",
    "    elif avg_doc_length < 2000:\n",
    "        return 1500  # Medium chunks\n",
    "    else:\n",
    "        return 2000  # Larger chunks for long docs\n",
    "\n",
    "def chunk_documents_enhanced(documents: List, chunk_size: int = None, chunk_overlap: int = None) -> List:\n",
    "    \"\"\"\n",
    "    Enhanced document chunking with cleaning and optimization\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to chunk\n",
    "        chunk_size: Override automatic chunk size detection\n",
    "        chunk_overlap: Chunk overlap size\n",
    "        \n",
    "    Returns:\n",
    "        List: Processed and chunked documents\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced document chunking...\")\n",
    "    print(f\"Original documents: {len(documents)}\")\n",
    "    \n",
    "    # Step 1: Clean document content\n",
    "    print(\"Step 1: Cleaning document content...\")\n",
    "    cleaned_docs = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        if i % 100 == 0:  # Progress update\n",
    "            print(f\"  Cleaned {i}/{len(documents)} documents...\")\n",
    "        \n",
    "        try:\n",
    "            # Clean content\n",
    "            cleaned_content = clean_content(doc.page_content)\n",
    "            \n",
    "            # Filter quality\n",
    "            if filter_quality_content(cleaned_content):\n",
    "                doc.page_content = cleaned_content\n",
    "                cleaned_docs.append(doc)\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to clean document {i}: {e}\")\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"  ‚úì Cleaned: {len(cleaned_docs)} documents\")\n",
    "    print(f\"  ‚úó Skipped: {skipped_count} low-quality documents\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del documents\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 2: Optimize chunk parameters\n",
    "    print(\"Step 2: Optimizing chunk parameters...\")\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = optimize_chunk_size_for_content(cleaned_docs)\n",
    "    \n",
    "    if chunk_overlap is None:\n",
    "        chunk_overlap = min(300, chunk_size // 5)  # 20% overlap, max 300 chars\n",
    "    \n",
    "    print(f\"  Using chunk_size: {chunk_size}\")\n",
    "    print(f\"  Using chunk_overlap: {chunk_overlap}\")\n",
    "    \n",
    "    # Step 3: Create text splitter with technical content awareness\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",      # Paragraph breaks\n",
    "            \"\\n\",        # Line breaks\n",
    "            \"```\",       # Code blocks\n",
    "            \"def \",      # Function definitions\n",
    "            \"class \",    # Class definitions\n",
    "            \"import \",   # Import statements\n",
    "            \". \",        # Sentences\n",
    "            \", \",        # Clauses\n",
    "            \" \",         # Words\n",
    "            \"\",          # Characters\n",
    "        ],\n",
    "        keep_separator=True,\n",
    "    )\n",
    "    \n",
    "    # Step 4: Split documents into chunks\n",
    "    print(\"Step 3: Creating chunks...\")\n",
    "    chunks = []\n",
    "    \n",
    "    batch_size = 50  # Process in batches to manage memory\n",
    "    for i in range(0, len(cleaned_docs), batch_size):\n",
    "        batch = cleaned_docs[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_chunks = text_splitter.split_documents(batch)\n",
    "            chunks.extend(batch_chunks)\n",
    "            \n",
    "            if i % (batch_size * 10) == 0:  # Progress update every 500 docs\n",
    "                print(f\"  Processed {i}/{len(cleaned_docs)} documents...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to chunk batch {i//batch_size}: {e}\")\n",
    "    \n",
    "    # Step 5: Final quality filter on chunks\n",
    "    print(\"Step 4: Final quality filtering...\")\n",
    "    quality_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if (len(chunk.page_content.strip()) >= 100 and  # Minimum meaningful length\n",
    "            filter_quality_content(chunk.page_content, min_length=100)):\n",
    "            quality_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"  Original documents: {len(documents) if 'documents' in locals() else 'N/A'}\")\n",
    "    print(f\"  Cleaned documents: {len(cleaned_docs)}\")\n",
    "    print(f\"  Total chunks created: {len(chunks)}\")\n",
    "    print(f\"  Quality chunks: {len(quality_chunks)}\")\n",
    "    print(f\"  Average chunk length: {sum(len(c.page_content) for c in quality_chunks) / len(quality_chunks):.0f} characters\")\n",
    "    \n",
    "    print(\"‚úì Enhanced document chunking complete!\")\n",
    "    \n",
    "    return quality_chunks\n",
    "\n",
    "# Main execution with error handling and monitoring\n",
    "try:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED TEXT CHUNKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if documents exist from previous cell\n",
    "    if 'documents' not in locals():\n",
    "        print(\"‚ùå Error: 'documents' not found. Please run the document loading cell first.\")\n",
    "    else:\n",
    "        # Run enhanced chunking\n",
    "        chunks = chunk_documents_enhanced(\n",
    "            documents,\n",
    "            chunk_size=1500,    # Increased from 1000 for technical content\n",
    "            chunk_overlap=300   # Increased overlap for better context\n",
    "        )\n",
    "        \n",
    "        # Save a sample chunk for inspection\n",
    "        if chunks:\n",
    "            print(f\"\\nüìÑ Sample chunk preview:\")\n",
    "            print(f\"Content: {chunks[0].page_content[:300]}...\")\n",
    "            print(f\"Metadata: {chunks[0].metadata}\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ Text chunking completed successfully!\")\n",
    "        print(f\"Ready for vector store creation with {len(chunks)} high-quality chunks\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Critical error in text chunking: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Fallback to basic chunking if enhanced version fails\n",
    "    print(\"\\nüîÑ Falling back to basic chunking...\")\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"‚úÖ Basic chunking completed: {len(chunks)} chunks created\")\n",
    "    except Exception as fallback_error:\n",
    "        print(f\"‚ùå Fallback chunking also failed: {fallback_error}\")\n",
    "        chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06886624-5f28-442b-9f29-0800ee07d118",
   "metadata": {},
   "source": [
    "## FAISS Vector Store Creation\n",
    "\n",
    "This cell creates a FAISS vector store from the processed document chunks using batch processing for memory efficiency.\n",
    "The system uses high-quality sentence transformers embeddings and provides comprehensive error handling and progress tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Batch processing to handle large datasets efficiently\n",
    "- High-quality sentence-transformers/all-mpnet-base-v2 embeddings\n",
    "- Automatic cleanup of existing vector stores\n",
    "- Memory management with garbage collection between batches\n",
    "- Detailed progress reporting and vector store statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d601af7-5393-48a6-aeb8-06342174e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VECTOR STORE CREATION\n",
      "============================================================\n",
      "Creating vector store from 21312 chunks...\n",
      "Loading embeddings model...\n",
      "‚úì Embeddings model loaded\n",
      "Deleting existing vector store at 'gpu_datascience_vectorstore'...\n",
      "‚úì Deleted existing vector store\n",
      "Creating new vector store...\n",
      "Processing 21312 chunks in batches of 1000\n",
      "  Processing batch 1/22 (1000 chunks)...\n",
      "    ‚úì Created initial vector store with 1000 documents\n",
      "  Processing batch 2/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 2000)\n",
      "  Processing batch 3/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 3000)\n",
      "  Processing batch 4/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 4000)\n",
      "  Processing batch 5/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 5000)\n",
      "  Processing batch 6/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 6000)\n",
      "  Processing batch 7/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 7000)\n",
      "  Processing batch 8/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 8000)\n",
      "  Processing batch 9/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 9000)\n",
      "  Processing batch 10/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 10000)\n",
      "  Processing batch 11/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 11000)\n",
      "  Processing batch 12/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 12000)\n",
      "  Processing batch 13/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 13000)\n",
      "  Processing batch 14/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 14000)\n",
      "  Processing batch 15/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 15000)\n",
      "  Processing batch 16/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 16000)\n",
      "  Processing batch 17/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 17000)\n",
      "  Processing batch 18/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 18000)\n",
      "  Processing batch 19/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 19000)\n",
      "  Processing batch 20/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 20000)\n",
      "  Processing batch 21/22 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 21000)\n",
      "  Processing batch 22/22 (312 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 21312)\n",
      "‚úì Batch processing complete: 21312 total vectors\n",
      "‚úì Vector store saved to 'gpu_datascience_vectorstore'\n",
      "\n",
      "‚úÖ Vector store ready!\n",
      "üìä Total vectors: 21312\n",
      "üìê Vector dimension: 768\n",
      "üíæ Saved to: gpu_datascience_vectorstore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_vector_store_batched(chunks, embeddings, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Create vector store with batch processing to handle memory efficiently\n",
    "    \n",
    "    Args:\n",
    "        chunks: Document chunks\n",
    "        embeddings: Embedding model\n",
    "        batch_size: Number of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(chunks)} chunks in batches of {batch_size}\")\n",
    "    \n",
    "    vector_store = None\n",
    "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        \n",
    "        print(f\"  Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)...\")\n",
    "        \n",
    "        try:\n",
    "            if vector_store is None:\n",
    "                # Create initial vector store from first batch\n",
    "                vector_store = FAISS.from_documents(batch, embeddings)\n",
    "                print(f\"    ‚úì Created initial vector store with {len(batch)} documents\")\n",
    "            else:\n",
    "                # Create temporary vector store for this batch\n",
    "                batch_store = FAISS.from_documents(batch, embeddings)\n",
    "                \n",
    "                # Merge with main vector store\n",
    "                vector_store.merge_from(batch_store)\n",
    "                print(f\"    ‚úì Merged batch into vector store (total: {vector_store.index.ntotal})\")\n",
    "                \n",
    "                # Clean up batch store\n",
    "                del batch_store\n",
    "                \n",
    "            # Garbage collection between batches\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Failed to process batch {batch_num}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if vector_store is None:\n",
    "        raise RuntimeError(\"Failed to create vector store from any batch\")\n",
    "    \n",
    "    print(f\"‚úì Batch processing complete: {vector_store.index.ntotal} total vectors\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VECTOR STORE CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if chunks exist\n",
    "if 'chunks' not in locals():\n",
    "    print(\"‚ùå Error: 'chunks' not found. Please run the text chunking cell first.\")\n",
    "else:\n",
    "    print(f\"Creating vector store from {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Create embeddings (all-mpnet-base-v2 provides best quality for technical content)\n",
    "    print(\"Loading embeddings model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"  # Better quality than all-MiniLM-L6-v2\n",
    "    )\n",
    "    print(\"‚úì Embeddings model loaded\")\n",
    "    \n",
    "    # Always delete existing vector store if it exists\n",
    "    store_path = \"gpu_datascience_vectorstore\"\n",
    "    \n",
    "    if os.path.exists(store_path):\n",
    "        print(f\"Deleting existing vector store at '{store_path}'...\")\n",
    "        try:\n",
    "            shutil.rmtree(store_path)\n",
    "            print(f\"‚úì Deleted existing vector store\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not delete existing store: {e}\")\n",
    "    \n",
    "    # Create new vector store\n",
    "    print(\"Creating new vector store...\")\n",
    "    vector_store = create_vector_store_batched(chunks, embeddings)\n",
    "    \n",
    "    # Save vector store\n",
    "    vector_store.save_local(store_path)\n",
    "    print(f\"‚úì Vector store saved to '{store_path}'\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector store ready!\")\n",
    "    print(f\"üìä Total vectors: {vector_store.index.ntotal}\")\n",
    "    print(f\"üìê Vector dimension: {vector_store.index.d}\")\n",
    "    print(f\"üíæ Saved to: {store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64973064-a2ff-412d-b7f4-93e4f08b4308",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "This cell loads the Llama3-8B-Instruct model and tokenizer for response generation. The model is loaded with 16-bit precision and automatic device mapping for optimal GPU utilization.\n",
    "\n",
    "**Configuration:**\n",
    "- Model: Llama3-8B-Instruct from local path\n",
    "- Precision: torch.float16 for memory efficiency\n",
    "- Device mapping: Automatic GPU allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d18fcdc7-a4b1-4bc0-98a2-158871def57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262f82d8b2ad4cc48db222aeb9fe7f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load your local Llama3 model (using your existing setup)\n",
    "model_name = '/data/datasets/community/huggingface/Llama3-70b-instruct/'  # Your model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fba4d-850d-4b0d-8d13-ee18f38e769c",
   "metadata": {},
   "source": [
    "## Enhanced Document Retrieval and Grading System\n",
    "\n",
    "This cell implements a sophisticated document retrieval system with intelligent grading, filtering, and context generation. The system uses multiple retrieval attempts with document exclusion to find the most relevant sources and generates enhanced context for response generation.\n",
    "\n",
    "**Core Components:**\n",
    "- Multi-attempt retrieval with document exclusion tracking\n",
    "- LLM-based document grading with strict relevance criteria\n",
    "- Quality filtering to keep only highly and somewhat relevant documents\n",
    "- Enhanced context generation with refined queries and generation instructions\n",
    "- Robust error handling and fallback mechanisms for parsing failures\n",
    "- Adaptive search expansion when insufficient relevant documents are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0022e09-7369-4b1c-9333-1ed7914dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Utility function to get document ID\n",
    "def get_doc_id(doc):\n",
    "    \"\"\"Generate unique ID for a document\"\"\"\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    content_snippet = doc.page_content[:100].replace('\\n', ' ').strip()\n",
    "    return f\"{source}_{hash(content_snippet)}\"\n",
    "    \n",
    "def retrieve_node(question: str, vector_store, model, tokenizer, max_attempts: int = 3,\n",
    "                 k=5, exclude_ids=None, attempt_number=0, return_doc_ids=False):\n",
    "    \"\"\"\n",
    "    Enhanced retrieve node with document exclusion and tracking\n",
    "    \n",
    "    Args:\n",
    "        return_doc_ids (bool): If True, returns (context, doc_ids), else just context\n",
    "    \"\"\"\n",
    "    # Declare global variable at the top\n",
    "    global _current_doc_ids\n",
    "    \n",
    "    print(f\"ENHANCED RETRIEVE NODE: {question}\")\n",
    "    print(\"Target: At least 2 highly_relevant + drop not_relevant documents\")\n",
    "    \n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f\"\\n--- Retrieval Attempt {attempt} ---\")\n",
    "        \n",
    "        # Step 1: Retrieve documents with exclusion logic\n",
    "        try:\n",
    "            # NEW: Add exclusion logic at the beginning\n",
    "            if exclude_ids:\n",
    "                print(f\"Excluding {len(exclude_ids)} previously retrieved documents\")\n",
    "                # Get more documents to account for exclusions\n",
    "                all_documents = vector_store.similarity_search(question, k=k*3)\n",
    "                filtered_docs = []\n",
    "                excluded_count = 0\n",
    "                \n",
    "                for doc in all_documents:\n",
    "                    doc_id = get_doc_id(doc)\n",
    "                    if doc_id not in exclude_ids:\n",
    "                        filtered_docs.append(doc)\n",
    "                        if len(filtered_docs) >= k:\n",
    "                            break\n",
    "                    else:\n",
    "                        excluded_count += 1\n",
    "                \n",
    "                documents = filtered_docs\n",
    "                print(f\"Excluded {excluded_count} documents, using {len(documents)} new documents\")\n",
    "            else:\n",
    "                # CHANGED: Use k parameter instead of hardcoded 5\n",
    "                documents = vector_store.similarity_search(question, k=k)\n",
    "                \n",
    "            print(f\"‚úì Retrieved {len(documents)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Retrieval failed: {e}\")\n",
    "            if attempt >= max_attempts:\n",
    "                return f\"Unable to retrieve documents for: {question}\"\n",
    "            continue\n",
    "        \n",
    "        if len(documents) < 3:\n",
    "            print(f\"‚ö†Ô∏è  Only {len(documents)} documents found\")\n",
    "            if attempt >= max_attempts:\n",
    "                if documents:\n",
    "                    break\n",
    "                else:\n",
    "                    return f\"No documents found for: {question}\"\n",
    "        \n",
    "        # Step 2: Grade documents (YOUR EXISTING LOGIC)\n",
    "        grades = grade_documents_strict(question, documents, model, tokenizer)\n",
    "        \n",
    "        # Step 3: Filter out irrelevant documents (YOUR EXISTING LOGIC)\n",
    "        relevant_docs = filter_and_show_documents(documents, grades)\n",
    "        dropped_count = len(documents) - len(relevant_docs)\n",
    "        print(f\"  Immediately filtered: Kept {len(relevant_docs)}, dropped {dropped_count}\")\n",
    "        \n",
    "        # Step 4: Analyze the FILTERED results (YOUR EXISTING LOGIC)\n",
    "        highly_relevant_count = count_by_relevance(grades, \"highly_relevant\")\n",
    "        somewhat_relevant_count = count_by_relevance(grades, \"somewhat_relevant\")\n",
    "        \n",
    "        print(f\"Grading Analysis (after filtering):\")\n",
    "        print(f\"  Highly relevant: {highly_relevant_count}\")\n",
    "        print(f\"  Somewhat relevant: {somewhat_relevant_count}\") \n",
    "        print(f\"  Total relevant kept: {len(relevant_docs)}\")\n",
    "        \n",
    "        # Show breakdown of what we're keeping (YOUR EXISTING LOGIC)\n",
    "        for grade in grades.get(\"document_grades\", []):\n",
    "            relevance = grade.get(\"relevance\", \"unknown\")\n",
    "            if relevance in [\"highly_relevant\", \"somewhat_relevant\"]:\n",
    "                doc_idx = grade.get(\"doc_index\", \"?\")\n",
    "                reason = grade.get(\"reason\", \"\")[:50]\n",
    "                print(f\"  Kept Doc {doc_idx}: {relevance.upper()} - {reason}...\")\n",
    "        \n",
    "        # Step 5: Check quality requirements (YOUR EXISTING LOGIC)\n",
    "        if highly_relevant_count >= 1 and len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì SUCCESS: Found {highly_relevant_count} highly relevant + {len(relevant_docs)} total relevant\")\n",
    "            \n",
    "            # Generate enhanced context with filtered documents (YOUR EXISTING LOGIC)\n",
    "            enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs we're using\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            \n",
    "            print(f\"‚úì Enhanced context generated with {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids\n",
    "            else:\n",
    "                # For LangGraph compatibility, store in global variable\n",
    "                _current_doc_ids = used_doc_ids\n",
    "                return enhanced_context\n",
    "        \n",
    "        elif len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì ACCEPTABLE: Found {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs we're using\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            \n",
    "            print(f\"‚úì Enhanced context generated with {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids\n",
    "            else:\n",
    "                # For LangGraph compatibility, store in global variable\n",
    "                _current_doc_ids = used_doc_ids\n",
    "                return enhanced_context\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚úó INSUFFICIENT: Only {len(relevant_docs)} relevant documents after filtering\")\n",
    "            \n",
    "            if attempt >= max_attempts:\n",
    "                print(\"Max attempts reached\")\n",
    "                if relevant_docs:\n",
    "                    enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "                    \n",
    "                    # Track the document IDs we're using\n",
    "                    used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "                    \n",
    "                    print(f\"‚ö†Ô∏è  Using {len(relevant_docs)} available relevant documents\")\n",
    "                    \n",
    "                    if return_doc_ids:\n",
    "                        return enhanced_context, used_doc_ids\n",
    "                    else:\n",
    "                        # For LangGraph compatibility, store in global variable\n",
    "                        _current_doc_ids = used_doc_ids\n",
    "                        return enhanced_context\n",
    "                else:\n",
    "                    if return_doc_ids:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", []\n",
    "                    else:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\"\n",
    "            else:\n",
    "                print(f\"Will search for more documents in next attempt...\")\n",
    "                # Continue with expanded search\n",
    "                k = min(k + 3, 15)\n",
    "    \n",
    "    if return_doc_ids:\n",
    "        return f\"Failed to find adequate documents for: {question}\", []\n",
    "    else:\n",
    "        return f\"Failed to find adequate documents for: {question}\"\n",
    "\n",
    "def grade_documents_strict(question: str, documents: List, model, tokenizer) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Grade documents with balanced criteria - not too strict\n",
    "    \"\"\"\n",
    "    print(f\"---DOCUMENT GRADING ({len(documents)} documents)---\")\n",
    "    \n",
    "    if not documents:\n",
    "        return {\"document_grades\": [], \"overall_assessment\": \"poor\"}\n",
    "    \n",
    "    # Grade only first 5 documents to avoid overwhelming the model\n",
    "    docs_to_grade = documents[:5]\n",
    "    docs_text = \"\"\n",
    "    for i, doc in enumerate(docs_to_grade, 1):\n",
    "        content = doc.page_content[:400]\n",
    "        docs_text += f\"=== DOCUMENT {i} ===\\n{content}...\\n\\n\"\n",
    "    \n",
    "    # More balanced grading prompt\n",
    "    system_prompt = f\"\"\"You are grading document relevance for a technical question. Grade each document:\n",
    "\n",
    "GRADING CRITERIA:\n",
    "- highly_relevant: Document contains specific, detailed information that directly addresses the question\n",
    "- somewhat_relevant: Document contains related information that provides useful context\n",
    "- not_relevant: Document is completely off-topic or unhelpful\n",
    "\n",
    "Be reasonable - if a document discusses the same technology area, it's at least somewhat_relevant.\n",
    "\n",
    "Respond with ONLY this JSON format:\n",
    "{{\"document_grades\": [{{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 2, \"relevance\": \"highly_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 3, \"relevance\": \"not_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 4, \"relevance\": \"somewhat_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 5, \"relevance\": \"highly_relevant\", \"reason\": \"brief reason\"}}], \"overall_assessment\": \"good\"}}\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"QUESTION: {question}\n",
    "\n",
    "DOCUMENTS TO GRADE:\n",
    "{docs_text}\n",
    "\n",
    "Grade ALL 5 documents. Be reasonable - related technical content should be at least somewhat_relevant.\"\"\"\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1200,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        print(f\"  Grading response: {response[:150]}...\")\n",
    "        \n",
    "        # Parse grades for the 5 documents we graded\n",
    "        grades = parse_grades_safely(response, len(docs_to_grade))\n",
    "        \n",
    "        # For documents beyond the first 5, mark as not_relevant\n",
    "        if len(documents) > 5:\n",
    "            additional_grades = []\n",
    "            for i in range(6, len(documents) + 1):\n",
    "                additional_grades.append({\n",
    "                    \"doc_index\": i,\n",
    "                    \"relevance\": \"not_relevant\", \n",
    "                    \"reason\": \"not graded - beyond top 5\"\n",
    "                })\n",
    "            grades[\"document_grades\"].extend(additional_grades)\n",
    "        \n",
    "        return grades\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Grading failed: {e}\")\n",
    "        return create_fallback_grades(len(documents))\n",
    "\n",
    "def parse_grades_safely(response: str, num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse grading response safely without regex issues\n",
    "    \"\"\"\n",
    "    # Try to extract JSON\n",
    "    json_start = response.find('{')\n",
    "    json_end = response.rfind('}') + 1\n",
    "    \n",
    "    if json_start != -1 and json_end > json_start:\n",
    "        try:\n",
    "            json_text = response[json_start:json_end]\n",
    "            grades = json.loads(json_text)\n",
    "            \n",
    "            if \"document_grades\" in grades and isinstance(grades[\"document_grades\"], list):\n",
    "                return complete_missing_grades(grades, num_docs)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Fallback parsing\n",
    "    return extract_grades_from_text(response, num_docs)\n",
    "\n",
    "def extract_grades_from_text(text: str, num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract grades from text when JSON parsing fails\n",
    "    \"\"\"\n",
    "    grades = []\n",
    "    \n",
    "    # Look for patterns like \"doc_index\": 1, \"relevance\": \"highly_relevant\"\n",
    "    for i in range(1, num_docs + 1):\n",
    "        if f'\"doc_index\": {i}' in text or f'\"doc_index\":{i}' in text:\n",
    "            if 'highly_relevant' in text:\n",
    "                relevance = \"highly_relevant\"\n",
    "            elif 'somewhat_relevant' in text:\n",
    "                relevance = \"somewhat_relevant\"\n",
    "            else:\n",
    "                relevance = \"not_relevant\"\n",
    "            \n",
    "            grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": relevance,\n",
    "                \"reason\": \"extracted from text\"\n",
    "            })\n",
    "        else:\n",
    "            grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": \"not_relevant\",\n",
    "                \"reason\": \"not found in response\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"document_grades\": grades,\n",
    "        \"overall_assessment\": \"acceptable\"\n",
    "    }\n",
    "\n",
    "def complete_missing_grades(grades: Dict[str, Any], expected_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fill in missing document grades\n",
    "    \"\"\"\n",
    "    doc_grades = grades.get(\"document_grades\", [])\n",
    "    graded_docs = {grade.get(\"doc_index\"): grade for grade in doc_grades \n",
    "                   if isinstance(grade.get(\"doc_index\"), int)}\n",
    "    \n",
    "    complete_grades = []\n",
    "    for i in range(1, expected_docs + 1):\n",
    "        if i in graded_docs:\n",
    "            grade = graded_docs[i]\n",
    "            relevance = grade.get(\"relevance\", \"not_relevant\")\n",
    "            if relevance not in [\"highly_relevant\", \"somewhat_relevant\", \"not_relevant\"]:\n",
    "                relevance = \"not_relevant\"\n",
    "            complete_grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": relevance,\n",
    "                \"reason\": grade.get(\"reason\", \"graded by model\")\n",
    "            })\n",
    "        else:\n",
    "            complete_grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": \"not_relevant\",\n",
    "                \"reason\": \"not graded by model\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"document_grades\": complete_grades,\n",
    "        \"overall_assessment\": grades.get(\"overall_assessment\", \"acceptable\")\n",
    "    }\n",
    "\n",
    "def create_fallback_grades(num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create conservative fallback grades\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"document_grades\": [\n",
    "            {\"doc_index\": i, \"relevance\": \"not_relevant\", \"reason\": \"grading failed\"}\n",
    "            for i in range(1, num_docs + 1)\n",
    "        ],\n",
    "        \"overall_assessment\": \"poor\"\n",
    "    }\n",
    "\n",
    "def count_by_relevance(grades: Dict[str, Any], relevance_level: str) -> int:\n",
    "    \"\"\"\n",
    "    Count documents by relevance level\n",
    "    \"\"\"\n",
    "    return sum(1 for g in grades.get(\"document_grades\", [])\n",
    "               if g.get(\"relevance\") == relevance_level)\n",
    "\n",
    "def filter_and_show_documents(documents: List, grades: Dict[str, Any]) -> List:\n",
    "    \"\"\"\n",
    "    Filter documents and show what's being kept/dropped\n",
    "    \"\"\"\n",
    "    relevant_docs = []\n",
    "    \n",
    "    print(\"  Document filtering:\")\n",
    "    for grade in grades.get(\"document_grades\", []):\n",
    "        doc_index = grade.get(\"doc_index\", 0) - 1\n",
    "        relevance = grade.get(\"relevance\", \"not_relevant\")\n",
    "        \n",
    "        if 0 <= doc_index < len(documents):\n",
    "            if relevance in [\"highly_relevant\", \"somewhat_relevant\"]:\n",
    "                relevant_docs.append(documents[doc_index])\n",
    "                print(f\"    ‚úì Keeping Doc {doc_index + 1}: {relevance}\")\n",
    "            else:\n",
    "                print(f\"    ‚úó Dropping Doc {doc_index + 1}: {relevance}\")\n",
    "    \n",
    "    dropped_count = len(documents) - len(relevant_docs)\n",
    "    print(f\"  Result: Kept {len(relevant_docs)} relevant, dropped {dropped_count} irrelevant\")\n",
    "    return relevant_docs\n",
    "\n",
    "def create_clean_context(question: str, relevant_docs: List, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Create clean, formatted context without regex issues\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING ENHANCED CONTEXT---\")\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return f\"No relevant documents found for: {question}\"\n",
    "    \n",
    "    # Create knowledge base section\n",
    "    knowledge_base = \"=== KNOWLEDGE BASE ===\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        content = doc.page_content.strip()\n",
    "        # Clean whitespace without complex regex\n",
    "        content = ' '.join(content.split())\n",
    "        knowledge_base += f\"### Source {i}:\\n{content}\\n\\n\"\n",
    "    \n",
    "    # Generate refined query\n",
    "    refined_query = generate_refined_query(question, model, tokenizer)\n",
    "    \n",
    "    # Create instructions\n",
    "    instructions = get_generation_instructions(question)\n",
    "    \n",
    "    # Assemble context\n",
    "    enhanced_context = f\"\"\"{knowledge_base}=== ENHANCED QUERY ===\n",
    "{refined_query}\n",
    "\n",
    "=== GENERATION INSTRUCTIONS ===\n",
    "{instructions}\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Enhanced context generated with clean formatting\")\n",
    "    return enhanced_context\n",
    "\n",
    "def generate_refined_query(question: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Generate refined query without regex issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Refine this technical question to be more specific and comprehensive:\n",
    "\n",
    "Original: {question}\n",
    "\n",
    "Create a refined version that asks for more detailed information. Respond with ONLY the refined question.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You refine technical questions to be more comprehensive.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        refined = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean without regex\n",
    "        refined = refined.strip(' \\'\"')\n",
    "        return refined if refined else question\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Query refinement failed: {e}\")\n",
    "        return question\n",
    "\n",
    "def get_generation_instructions(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Get generation instructions based on question type\n",
    "    \"\"\"\n",
    "    q_lower = question.lower()\n",
    "    \n",
    "    base = \"\"\"Using the provided sources, generate a comprehensive response that:\n",
    "\n",
    "1. **Directly addresses the question**\n",
    "2. **Uses source material effectively** \n",
    "3. **Maintains technical accuracy**\"\"\"\n",
    "    \n",
    "    if any(word in q_lower for word in ['write', 'code', 'implement', 'create']):\n",
    "        specific = \"\"\"\n",
    "4. **Provide working code examples**\n",
    "5. **Explain code structure and logic**\n",
    "6. **Include usage examples**\n",
    "7. **Mention best practices**\"\"\"\n",
    "    elif any(word in q_lower for word in ['optimize', 'performance', 'improve']):\n",
    "        specific = \"\"\"\n",
    "4. **Detail optimization techniques**\n",
    "5. **Discuss performance considerations**\n",
    "6. **Include measurement methods**\n",
    "7. **Give practical recommendations**\"\"\"\n",
    "    else:\n",
    "        specific = \"\"\"\n",
    "4. **Build from fundamentals**\n",
    "5. **Use clear examples**\n",
    "6. **Explain relationships**\n",
    "7. **Summarize key points**\"\"\"\n",
    "    \n",
    "    return base + specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17f8c-9778-4d91-8f2d-e8babbbe9cb2",
   "metadata": {},
   "source": [
    "## Test Cases for Verifying Retrieve Node\n",
    "\n",
    "This block presents a handful of questions pertaining to the knowledge base ingested.\n",
    "This provides an opportunity to verify if the node runs correctly\n",
    "\n",
    "**Test Topics**\n",
    "- CUDA Programming\n",
    "- CuPy\n",
    "- PyTorch\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eb4520c-2f69-4f33-a80b-3035cc1ddc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RETRIEVE NODE\n",
      "================================================================================\n",
      "\n",
      "[TEST  1] How to optimize CUDA memory usage?\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: How to optimize CUDA memory usage?\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"general CUDA memory overview\"}, {\"doc_index\": 2, \"relevance\": \"high...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - general CUDA memory overview...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - example of optimizing CUDA memory usage...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - related to CUDA memory management...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - example of custom CUDA memory allocator...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - general CUDA architecture and kernel execution...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 4,643 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: Learn-CUDA-Programming/Chapter02/02_memory_overview at master ¬∑ PacktPublishing/Learn-CUDA-Programming ¬∑ GitHub Skip to content You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh y...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  2] CuPy array operations best practices\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: CuPy array operations best practices\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"discusses device management, related to array operations\"}, {\"doc_i...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - discusses device management, related to array oper...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - directly addresses best practices for array operat...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - mentions cupy.ndarray, related to array operations...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - describes cupy.ndarray, a key component of array o...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - provides API reference for CuPy, including array o...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,679 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . ', '', '## Moving Data from Current Device', '', 'In general, CuPy functions expect that the array is on the same device as the current one. Similar to passing data from CPU to GPU or vice versa, passing an array stored on a non-current device may impact perfo...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  3] PyTorch GPU training optimization techniques\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: PyTorch GPU training optimization techniques\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"mentions profiling, but not specifically for GPU training optimizat...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úó Dropping Doc 3: not_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - mentions profiling, but not specifically for GPU t...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - provides specific techniques for PyTorch GPU train...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - example code for PyTorch model training, but no sp...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - discusses PyTorch's GPU support and optimization...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 4,813 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Prepare the data and model 2. Use profiler to record execution events 3. Run the profiler 4. Use TensorBoard to view results and analyze model performance 5. Improve performance with the help of profiler 6. Analyze performance with other advanced features 7. A...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  4] RAPIDS cuDF dataframe operations tutorial\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: RAPIDS cuDF dataframe operations tutorial\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"mentions cuDF installation and tutorial notebook\"}, {\"doc_index\": 2...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 1\n",
      "  Somewhat relevant: 4\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - mentions cuDF installation and tutorial notebook...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - provides tutorial on using cuDF on Colab...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - provides general info about cuDF and RAPIDS...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - introduces cuDF as a DataFrame library for GPU-acc...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - introduces RAPIDS framework, which includes cuDF...\n",
      "‚úì SUCCESS: Found 1 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,598 characters\n",
      "  - Sources: 6\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: import cudf completes successfully in a new code block, and then you are ready to go. If you run into any trouble, please reach out in the RAPIDS Slack and we‚Äôll help you get things working correctly. Running 10 minutes to cuDF on Colab Now that you have a worki...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  5] Numba CUDA kernel programming examples\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Numba CUDA kernel programming examples\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"provides Numba CUDA examples\"}, {\"doc_index\": 2, \"relevance\": \"highly...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 3\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - provides Numba CUDA examples...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - demonstrates Numba CUDA kernel implementation...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - discusses CUDA kernels, but not specifically Numba...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - outdated Numba CUDA documentation, but still relev...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - explains creating a kernel function with @cuda.jit...\n",
      "‚úì SUCCESS: Found 3 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 4,373 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: Examples ‚Äî Numba 0+untagged.1510.g1e70d8c.dirty documentation Numba for CUDA GPUs Examples View page source ExamplesÔÉÅ CUDA Built-in Target deprecation notice The CUDA target built-in to Numba is deprecated, with further development moved to the NVIDIA numba-cuda...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  6] TensorFlow GPU memory optimization strategies\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: TensorFlow GPU memory optimization strategies\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"mentions memory optimization, but not specifically for TensorFlow G...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úó Dropping Doc 3: not_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - mentions memory optimization, but not specifically...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - provides specific TensorFlow method for controllin...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - mentions monitoring GPU memory usage and troublesh...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - provides specific TensorFlow code for enabling mem...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,552 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Say you have a model and you‚Äôre interested in ways to optimize memory to avoid Out of Memory (OOM) errors or simply to ooze more out of your GPU. Well, you _might_ be in luck (if gradients take up a portion of your memory and you do not need to do gradient acc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  7] JAX GPU acceleration tutorial examples\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: JAX GPU acceleration tutorial examples\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"not_relevant\", \"reason\": \"unrelated to JAX or GPU acceleration\"}, {\"doc_index\": 2, \"relevance\": \"h...\n",
      "  Document filtering:\n",
      "    ‚úó Dropping Doc 1: not_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 1\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - links to JAX documentation and developer resources...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - discusses JAX GPU memory allocation...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - uses JAX for parallelism and sharding...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - introduces GPU computing basics, related to JAX GP...\n",
      "‚úì SUCCESS: Found 1 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,022 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . We're currently working on covering JAX's ideas and capabilities in a more comprehensive and up-to-date paper. Reference documentation For details about the JAX API, see the reference documentation. For getting started as a JAX developer, see the developer doc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  8] GPU memory hierarchy architecture explanation\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: GPU memory hierarchy architecture explanation\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"describes specific components of GPU memory hierarchy\"}, {\"doc_index\"...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úó Dropping Doc 5: not_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - describes specific components of GPU memory hierar...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - explains GPU architecture and its components...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - discusses GPU kernel execution, but not directly r...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - talks about data transfer between CPU and GPU, rel...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,520 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . In the case of an NVIDIA GPU, the shared memory, the L1 cache and the Constant memory cache are within the streaming multiprocessor block. Hence they are faster than the L2 cache, and GPU RAM', '- Textures - Read-only memory optimized for filtering, interpolat...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  9] Implement GPU-accelerated matrix operations\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Implement GPU-accelerated matrix operations\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"discusses matrix multiplication and GPU optimizations\"}, {\"doc_inde...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - discusses matrix multiplication and GPU optimizati...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - provides comprehensive theoretical and practical f...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - implements matrix multiplication using CUDA shared...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - discusses blocked algorithm for matrix multiplicat...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - mentions high-performance matrix multiplication in...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 7,220 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Matmul This section focuses on understanding the fundamentals and optimization of matrix multiplication (Matmul), a cornerstone operation in CUDA programming and high-performance computing (HPC). The provided resources cover both CPU implementations and GPU op...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST 10] Explain GPU compute capability differences\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Explain GPU compute capability differences\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"mentions parallel processing, a related concept to compute capabili...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - mentions parallel processing, a related concept to...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - explains data parallelism, a key aspect of GPU com...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - discusses GPU architecture, a related topic to com...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - compares GPU architecture to CPU, relevant to comp...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - lists GPU features that impact compute capability,...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,897 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Content\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . GPUs consist of thousands of processing cores that can execute instructions in parallel, making them ideal for computationally-intensive tasks.', '', '## GPU Strengths', '', 'GPUs have several strengths:', '- Parallel processing: GPUs can process thousands of ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TEST COMPLETE - 10 questions tested\n"
     ]
    }
   ],
   "source": [
    "# Test cases for retrieve node\n",
    "test_questions = [\n",
    "    # CUDA Programming Questions\n",
    "    \"How to optimize CUDA memory usage?\",\n",
    "    \n",
    "    # CuPy Questions  \n",
    "    \"CuPy array operations best practices\",\n",
    "    \n",
    "    # PyTorch GPU Questions\n",
    "    \"PyTorch GPU training optimization techniques\", \n",
    "    \n",
    "    # RAPIDS Questions\n",
    "    \"RAPIDS cuDF dataframe operations tutorial\",\n",
    "    \n",
    "    # Numba CUDA Questions\n",
    "    \"Numba CUDA kernel programming examples\",\n",
    "    \n",
    "    # TensorFlow GPU Questions\n",
    "    \"TensorFlow GPU memory optimization strategies\",\n",
    "    \n",
    "    # JAX GPU Questions\n",
    "    \"JAX GPU acceleration tutorial examples\",\n",
    "    \n",
    "    # General GPU Computing\n",
    "    \"GPU memory hierarchy architecture explanation\",\n",
    "    \n",
    "    # Code Generation Tests\n",
    "    \"Implement GPU-accelerated matrix operations\",\n",
    "    \n",
    "    # Content Generation Tests  \n",
    "    \"Explain GPU compute capability differences\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "print(\"TESTING RETRIEVE NODE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[TEST {i:2d}] {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        context = retrieve_node(question, vector_store,model,tokenizer)  # Use your vector_store here\n",
    "        \n",
    "        # Analyze results\n",
    "        context_length = len(context)\n",
    "        source_count = context.count(\"Source \")\n",
    "        has_kb = \"=== KNOWLEDGE BASE ===\" in context\n",
    "        is_code_request = \"code with: preamble\" in context\n",
    "        is_content_request = \"content with: background\" in context\n",
    "        \n",
    "        print(f\"‚úì Context generated successfully\")\n",
    "        print(f\"  - Length: {context_length:,} characters\")\n",
    "        print(f\"  - Sources: {source_count}\")\n",
    "        print(f\"  - KB Used: {'Yes' if has_kb else 'No'}\")\n",
    "        print(f\"  - Type: {'Code' if is_code_request else 'Content'}\")\n",
    "        \n",
    "        # Show preview\n",
    "        preview = context[:300].replace('\\n', ' ')\n",
    "        print(f\"  - Preview: {preview}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó ERROR: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTEST COMPLETE - {len(test_questions)} questions tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076626dd-a3f1-45b5-87d1-d05edd76868f",
   "metadata": {},
   "source": [
    "## Response Generation Pipeline\n",
    "\n",
    "This cell implements the core response generation function that combines retrieved context with the Llama3 model to produce comprehensive, source-based answers. The system includes enhanced prompting for accuracy and proper citation handling.\n",
    "\n",
    "**Key Features:**\n",
    "- Flexible context handling (retrieval or pre-provided)\n",
    "- Enhanced system prompt with strict source-only requirements\n",
    "- Chat history integration for follow-up questions\n",
    "- Structured response formatting with markdown headers\n",
    "- Code generation safety with hallucination prevention\n",
    "- Knowledge gap identification and research suggestions\n",
    "- Comprehensive error handling and response validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a2f70d-d4ad-4e17-b200-c9e85cc81e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, vector_store, model, tokenizer, max_tokens=2000, temperature=0.6,\n",
    "                     context=None, chat_context=\"\"):\n",
    "    \"\"\"\n",
    "    Complete GPU data science tutor pipeline\n",
    "    \n",
    "    Args:\n",
    "        question (str): User question\n",
    "        vector_store: Your vector store\n",
    "        model: Pre-loaded Llama3 model\n",
    "        tokenizer: Pre-loaded tokenizer\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Generation temperature\n",
    "        context (str): Pre-retrieved context (if None, will retrieve)\n",
    "        chat_context (str): Conversation history context\n",
    "        \n",
    "    Returns:\n",
    "        str: Final tutor response\n",
    "    \"\"\"\n",
    "    print(f\"GPU TUTOR: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # NEW: Step 1 - Use provided context or retrieve new context\n",
    "    if context is None:\n",
    "        print(\"Step 1: Retrieving and grading documents...\")\n",
    "        context = retrieve_node(question, vector_store, model, tokenizer)\n",
    "    else:\n",
    "        print(\"Step 1: Using provided context...\")\n",
    "    \n",
    "    # Step 2: Generate response\n",
    "    print(\"Step 2: Generating response...\")\n",
    "    \n",
    "#     # MODIFIED: Create system prompt for GPU data science tutor with chat context\n",
    "#     system_prompt = f\"\"\"You are an expert GPU data science tutor. Your job is to provide high-quality, accurate responses about GPU-accelerated computing, CUDA programming, and GPU data science frameworks.\n",
    "\n",
    "# {chat_context}\n",
    "\n",
    "# When generating code:\n",
    "# - Write complete, working code examples that can be run\n",
    "# - Include clear comments explaining each step and function\n",
    "# - Provide setup/import statements needed\n",
    "# - Add usage examples showing how to run the code\n",
    "# - Explain expected outputs or results\n",
    "# - Follow the specific tools/frameworks mentioned in the question\n",
    "# - Ensure code follows best practices for the requested technology\n",
    "# - If building on previous discussion, extend or modify previous examples appropriately\n",
    "\n",
    "# When generating content:\n",
    "# - Provide comprehensive background information\n",
    "# - Include practical examples and use cases\n",
    "# - Discuss future directions and trends\n",
    "# - Cite sources when possible\n",
    "\n",
    "# When generating explanations:\n",
    "# - Build explanations from the provided sources only\n",
    "# - Include practical examples and use cases\n",
    "# - Explain complex concepts step-by-step\n",
    "# - Connect to previous conversation context when relevant\n",
    "\n",
    "# If there's conversation history above, be aware of what was previously discussed and build upon it naturally. Reference previous topics when relevant, but focus primarily on answering the current question.\n",
    "\n",
    "# Use the provided context to inform your response, but also apply your knowledge to create complete, helpful answers.\"\"\"\n",
    "\n",
    "    # REPLACE the system_prompt in your generate_response function with this:\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert GPU data science tutor providing comprehensive, accurate responses about GPU-accelerated computing, CUDA programming, and GPU data science frameworks.\n",
    "\n",
    "{chat_context}\n",
    "\n",
    "CORE INSTRUCTIONS:\n",
    "- Use ONLY information from the provided context sources\n",
    "- Cite sources using [1], [2], [3] format\n",
    "- For follow-up questions, build directly on previous discussion\n",
    "- Provide maximum detail and explanation using available retrieved documents\n",
    "\n",
    "CODE GENERATION REQUIREMENTS:\n",
    "- ONLY use functions, classes, and methods demonstrated in the context sources\n",
    "- Include ONLY imports that are explicitly mentioned in context sources\n",
    "- If insufficient code examples in sources, explain what information is missing rather than hallucinating\n",
    "- State clearly when you lack enough information for complete working examples\n",
    "- Never invent function names, parameters, or library features not shown in sources\n",
    "\n",
    "CONTENT GENERATION APPROACH:\n",
    "- Analyze retrieved sources to identify available information and existing gaps\n",
    "- Explain concepts in maximum detail using only source information\n",
    "- Connect related concepts from different sources for comprehensive explanations\n",
    "- Build explanations progressively from basic to advanced topics\n",
    "- When encountering gaps, explicitly mention missing or incomplete topics\n",
    "- If sources contain conflicting information, acknowledge differences and cite specific sources\n",
    "\n",
    "RESPONSE STRUCTURE REQUIREMENTS:\n",
    "- Start with brief connection to previous discussion (if applicable)\n",
    "- Use clear markdown headers (##) to organize sections\n",
    "- Bold important terms and concepts (**term**) for emphasis\n",
    "- Structure as: Overview ‚Üí Key Concepts ‚Üí Implementation Details ‚Üí Code Examples (if sufficient sources)\n",
    "- Include \"Knowledge Gaps & Further Research\" section identifying:\n",
    "  * Topics mentioned but not fully covered in sources\n",
    "  * Related concepts for deeper understanding\n",
    "  * Specific areas needing additional research\n",
    "  * Suggested search terms for further exploration\n",
    "- End with practical usage guidance based on actual source content\n",
    "\n",
    "ACCURACY STANDARDS:\n",
    "- If sources lack specific details, state limitations explicitly\n",
    "- Acknowledge partial information when sources show incomplete examples\n",
    "- Identify knowledge gaps requiring additional research\n",
    "- Never guess syntax or availability when uncertain\"\"\"\n",
    "    # MODIFIED: Build the prompt with chat awareness\n",
    "    if chat_context:\n",
    "        user_prompt = f\"\"\"{chat_context}\n",
    "\n",
    "Context for current question:\n",
    "{context}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Please provide a comprehensive response that takes into account our previous conversation while focusing on the current question.\"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a comprehensive response based on the context above.\"\"\"\n",
    "\n",
    "    # Build messages (YOUR EXISTING LOGIC)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Generate response (YOUR EXISTING LOGIC)\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "        \n",
    "        # Extract response (YOUR EXISTING LOGIC)\n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        print(f\"‚úì Generated response ({len(response)} characters)\")\n",
    "        print(\"=\" * 80)\n",
    "        return response,context  # Return both response and context\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Generation failed: {e}\")\n",
    "        return f\"Sorry, I encountered an error generating a response: {e}\", context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c9891-1ada-49f5-aefb-5a1b478e48f8",
   "metadata": {},
   "source": [
    "## Test Cases for Generate Response Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48710765-56dc-4890-99a1-328776bc031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU TUTOR: why does gpu accelerated libraries perform better than cpu libraries for data science?\n",
      "================================================================================\n",
      "Step 1: Retrieving and grading documents...\n",
      "ENHANCED RETRIEVE NODE: why does gpu accelerated libraries perform better than cpu libraries for data science?\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"discusses GPU architecture, related to GPU acceleration\"}, {\"doc_in...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 3\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - discusses GPU architecture, related to GPU acceler...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - shows cuML accelerator for sklearn, umap, and hdbs...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - compares cuML performance with CPU, directly addre...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - explains data parallelism, a key concept in GPU pr...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - discusses potential speedups of GPU acceleration f...\n",
      "‚úì SUCCESS: Found 3 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "Step 2: Generating response...\n",
      "‚úì Generated response (4420 characters)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Overview\\n\\nModern Graphics Processing Units (GPUs) offer several architectural and computational advantages over Central Processing Units (CPUs) that enable GPU-accelerated libraries to outperform CPU-based libraries in various data science tasks. These advantages stem from the unique design of GPUs, which are optimized for parallel processing, massive data parallel throughput, and efficient memory hierarchy.\\n\\n## Key Concepts\\n\\n### Parallel Processing\\n\\nGPUs consist of multiple processor clusters, each containing numerous Streaming Multiprocessors (SMs) [Source 1]. This architecture enables GPUs to execute thousands of threads concurrently, making them ideal for data-parallel computations. In contrast, CPUs are designed for serial processing and are limited in their ability to handle parallel tasks.\\n\\n### Memory Hierarchy\\n\\nGPUs have a hierarchical memory structure, comprising small, fast caches (L1 and L2) and larger, slower global memory [Source 1]. This design allows GPUs to minimize memory access latency, which is critical for parallel computations. CPUs, on the other hand, rely on smaller, faster caches, but are limited by their serial processing nature.\\n\\n### Bandwidth\\n\\nGPUs have higher memory bandwidth than CPUs, enabling them to transfer large amounts of data quickly [Source 4]. This is essential for data-intensive tasks, such as linear algebra operations and neural network computations.\\n\\n### Data Parallelism\\n\\nGPU programming is centered around data parallelism, where the same calculation is performed on multiple data elements simultaneously [Source 4]. This approach is particularly effective for tasks like matrix multiplication, numerical integration, and statistical calculations.\\n\\n### Single Instruction, Multiple Data (SIMD)\\n\\nGPUs execute instructions in a SIMD fashion, where a single instruction is applied to multiple data elements [Source 4]. This leads to significant performance improvements for data-parallel computations.\\n\\n## Implementation Details\\n\\nGPU-accelerated libraries, such as cuDNN, TensorFlow, and PyTorch, leverage these architectural advantages to optimize performance-critical components of data science tasks. For instance:\\n\\n* Linear algebra operations: GPUs can perform matrix multiplications and other linear algebra operations much faster than CPUs due to their parallel processing capabilities.\\n* Neural network computations: GPUs can accelerate neural network computations by executing matrix multiplications and activations in parallel.\\n* Data preprocessing: GPUs can rapidly process large datasets, performing tasks like data augmentation and feature extraction in parallel.\\n\\n## Performance Metrics\\n\\nThe advantages of GPU-accelerated libraries translate to improved performance metrics, including:\\n\\n* **Speedup**: GPU-accelerated libraries can achieve speedups of 25x or more compared to CPU-based libraries for certain algorithms [Source 2].\\n* **Throughput**: GPUs can process large datasets much faster than CPUs, leading to increased throughput for data-intensive tasks.\\n* **Energy Efficiency**: GPUs can perform computations while consuming less power than CPUs, making them a more energy-efficient option for data science tasks.\\n\\n## Knowledge Gaps & Further Research\\n\\n* While the sources provide a solid foundation for understanding the advantages of GPU-accelerated libraries, there is a need for more research on optimizing GPU performance for specific data science tasks.\\n* Further investigation is required to explore the limitations of GPU-accelerated libraries and identify scenarios where CPU-based libraries may be more suitable.\\n* Additional research is needed to develop more efficient data transfer mechanisms between CPU and GPU memory.\\n\\n## Practical Usage Guidance\\n\\nWhen working with GPU-accelerated libraries, consider the following best practices:\\n\\n* Optimize data transfer between CPU and GPU memory to minimize overhead.\\n* Leverage data parallelism and SIMD instructions to maximize GPU utilization.\\n* Select algorithms that are well-suited for GPU acceleration, such as those with high computational complexity.\\n* Monitor GPU memory usage and adjust batch sizes accordingly to avoid memory constraints.\\n\\nBy understanding the architectural and computational advantages of modern GPUs, data scientists can harness the power of GPU-accelerated libraries to accelerate their data science tasks and achieve improved performance metrics.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"why does gpu accelerated libraries perform better than cpu libraries for data science?\"\n",
    "#context = retrieve_node(question, vector_store,model,tokenizer)\n",
    "response,context = generate_response(question, vector_store,model,tokenizer)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c74a2-c6da-4c19-9917-6f3ead287c2c",
   "metadata": {},
   "source": [
    "## Hallucination Detection and Correction\n",
    "\n",
    "This cell implements a hallucination grader that validates whether generated responses are properly grounded in the retrieved context. If hallucinations are detected, the system automatically regenerates the response with stricter parameters.\n",
    "\n",
    "**Key Features:**\n",
    "- Binary grounding assessment (GROUNDED/HALLUCINATED)\n",
    "- Automatic response regeneration when hallucinations detected\n",
    "- Configurable retry attempts with fallback mechanisms\n",
    "- Temperature-controlled regeneration for improved accuracy\n",
    "- Comprehensive error handling for grading failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97ea2bd0-b76d-434c-b2f9-3b496faad867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_grader(question, response, context, model, tokenizer, max_retries=2):\n",
    "    \"\"\"\n",
    "    Check if response is grounded in context, regenerate if hallucinated\n",
    "    \n",
    "    Args:\n",
    "        question (str): Original question\n",
    "        response (str): Generated response\n",
    "        context (str): Retrieved context\n",
    "        model: Llama3 model\n",
    "        tokenizer: Tokenizer\n",
    "        max_retries (int): Maximum regeneration attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Final response (original or regenerated)\n",
    "    \"\"\"\n",
    "    print(\"---HALLUCINATION GRADER---\")\n",
    "    \n",
    "    current_response = response\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        print(f\"Checking attempt {attempt + 1}...\")\n",
    "        \n",
    "        # Check if response is grounded\n",
    "        check_prompt = f\"\"\"CONTEXT:\n",
    "{context}\n",
    "\n",
    "RESPONSE:\n",
    "{current_response}\n",
    "\n",
    "Is this response grounded in the context? Answer only: GROUNDED or HALLUCINATED\"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "        \n",
    "        try:\n",
    "            # Generate grading\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "                eos_token_id=terminators, do_sample=False\n",
    "            )\n",
    "            \n",
    "            response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "            grade = tokenizer.decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "            \n",
    "            # Check result\n",
    "            if \"GROUNDED\" in grade:\n",
    "                print(f\"‚úì Response is grounded (attempt {attempt + 1})\")\n",
    "                return current_response\n",
    "            elif attempt < max_retries:\n",
    "                print(f\"‚úó Hallucination detected, regenerating...\")\n",
    "                # Regenerate response\n",
    "                current_response = generate_response(question, context, model, tokenizer, temperature = 0.5)\n",
    "            else:\n",
    "                print(f\"‚ö† Max retries reached, using last response\")\n",
    "                return current_response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Grading failed: {e}\")\n",
    "            return current_response\n",
    "    \n",
    "    return current_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08115b07-3301-4727-ba2c-87209d81177d",
   "metadata": {},
   "source": [
    "## Test Case for Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00013a9e-3bac-4121-b64c-9a0db6bdc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---HALLUCINATION GRADER---\n",
      "Checking attempt 1...\n",
      "‚úì Response is grounded (attempt 1)\n"
     ]
    }
   ],
   "source": [
    "graded_response = hallucination_grader(question, response, context, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d1b90-9caa-47fb-a96c-c2abe30477ed",
   "metadata": {},
   "source": [
    "## Answer Quality Assessment and Refinement\n",
    "\n",
    "This cell implements a strict answer grader that evaluates whether the generated response adequately addresses the original question. The system uses a teacher-student grading approach with automatic regeneration for inadequate responses.\n",
    "\n",
    "**Key Components:**\n",
    "- Strict grading criteria for answer adequacy assessment\n",
    "- Binary evaluation (ANSWERS/DOESNT_ANSWER) with detailed rubric\n",
    "- Multi-layered validation combining hallucination and answer grading\n",
    "- Progressive temperature adjustment for response variety during retries\n",
    "- Comprehensive error handling with fallback assumptions\n",
    "- Iterative refinement process with configurable retry limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2487682f-54d0-422d-8f10-e1b65aaa6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_grader(question, response, context, model, tokenizer, max_retries=2):\n",
    "    \"\"\"\n",
    "    Check if response answers the question, regenerate if not\n",
    "    \n",
    "    Args:\n",
    "        question (str): Original question\n",
    "        response (str): Response after hallucination grading\n",
    "        context (str): Retrieved context (passed from previous step)\n",
    "        model: Llama3 model\n",
    "        tokenizer: Tokenizer\n",
    "        max_retries (int): Maximum regeneration attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Final response that answers the question\n",
    "    \"\"\"\n",
    "    print(\"---ANSWER GRADER---\")\n",
    "    \n",
    "    current_response = response\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        print(f\"Checking if response answers question (attempt {attempt + 1})...\")\n",
    "        \n",
    "        # Check if response answers the question\n",
    "        answers_question = _check_if_answers_question(question, current_response, model, tokenizer)\n",
    "        \n",
    "        if answers_question:\n",
    "            print(f\"‚úì Response answers the question (attempt {attempt + 1})\")\n",
    "            return current_response\n",
    "        else:\n",
    "            print(f\"‚úó Response doesn't answer the question\")\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Regenerating response...\")\n",
    "                \n",
    "                # Reuse generation logic with higher temperature for variety\n",
    "                temp_response = generate_response(question, context, model, tokenizer, temperature=0.5)\n",
    "                current_response = hallucination_grader(question, temp_response, context, model, tokenizer)\n",
    "            else:\n",
    "                print(f\"‚ö† Max retries reached, using last response\")\n",
    "                return current_response\n",
    "    \n",
    "    return current_response\n",
    "\n",
    "def _check_if_answers_question(question, response, model, tokenizer):\n",
    "    \"\"\"Check if response actually answers the question with strict grading\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a strict teacher grading student responses. Your job is to determine if the student's answer actually addresses what was asked.\n",
    "\n",
    "GRADING CRITERIA:\n",
    "- Does the response directly address the specific question asked?\n",
    "- Does it provide the information the user was seeking?\n",
    "- Is it relevant to the topic being asked about?\n",
    "- If the question asks \"how to\" do something, does the response explain how?\n",
    "- If the question asks for examples, are examples provided?\n",
    "- If the question asks for comparisons, are comparisons made?\n",
    "\n",
    "BE STRICT. If the response is vague, off-topic, or doesn't directly answer what was asked, grade it as inadequate.\n",
    "\n",
    "Respond with ONLY one word: ANSWERS or DOESNT_ANSWER\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Grade this student response:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STUDENT RESPONSE: {response}\n",
    "\n",
    "Does this response directly and adequately answer the specific question that was asked?\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = tokenizer.decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        return \"ANSWERS\" in grade\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer check failed: {e}\")\n",
    "        return True  # Assume it answers if check fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688de31-2733-4fe9-8002-dad5adf9b4d0",
   "metadata": {},
   "source": [
    "## Test Case for Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60162701-b19b-44cd-8d50-a993eed02cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ANSWER GRADER---\n",
      "Checking if response answers question (attempt 1)...\n",
      "‚úì Response answers the question (attempt 1)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Overview\n",
       "\n",
       "Modern Graphics Processing Units (GPUs) offer several architectural and computational advantages over Central Processing Units (CPUs) that enable GPU-accelerated libraries to outperform CPU-based libraries in various data science tasks. These advantages stem from the unique design of GPUs, which are optimized for parallel processing, massive data parallel throughput, and efficient memory hierarchy.\n",
       "\n",
       "## Key Concepts\n",
       "\n",
       "### Parallel Processing\n",
       "\n",
       "GPUs consist of multiple processor clusters, each containing numerous Streaming Multiprocessors (SMs) [Source 1]. This architecture enables GPUs to execute thousands of threads concurrently, making them ideal for data-parallel computations. In contrast, CPUs are designed for serial processing and are limited in their ability to handle parallel tasks.\n",
       "\n",
       "### Memory Hierarchy\n",
       "\n",
       "GPUs have a hierarchical memory structure, comprising small, fast caches (L1 and L2) and larger, slower global memory [Source 1]. This design allows GPUs to minimize memory access latency, which is critical for parallel computations. CPUs, on the other hand, rely on smaller, faster caches, but are limited by their serial processing nature.\n",
       "\n",
       "### Bandwidth\n",
       "\n",
       "GPUs have higher memory bandwidth than CPUs, enabling them to transfer large amounts of data quickly [Source 4]. This is essential for data-intensive tasks, such as linear algebra operations and neural network computations.\n",
       "\n",
       "### Data Parallelism\n",
       "\n",
       "GPU programming is centered around data parallelism, where the same calculation is performed on multiple data elements simultaneously [Source 4]. This approach is particularly effective for tasks like matrix multiplication, numerical integration, and statistical calculations.\n",
       "\n",
       "### Single Instruction, Multiple Data (SIMD)\n",
       "\n",
       "GPUs execute instructions in a SIMD fashion, where a single instruction is applied to multiple data elements [Source 4]. This leads to significant performance improvements for data-parallel computations.\n",
       "\n",
       "## Implementation Details\n",
       "\n",
       "GPU-accelerated libraries, such as cuDNN, TensorFlow, and PyTorch, leverage these architectural advantages to optimize performance-critical components of data science tasks. For instance:\n",
       "\n",
       "* Linear algebra operations: GPUs can perform matrix multiplications and other linear algebra operations much faster than CPUs due to their parallel processing capabilities.\n",
       "* Neural network computations: GPUs can accelerate neural network computations by executing matrix multiplications and activations in parallel.\n",
       "* Data preprocessing: GPUs can rapidly process large datasets, performing tasks like data augmentation and feature extraction in parallel.\n",
       "\n",
       "## Performance Metrics\n",
       "\n",
       "The advantages of GPU-accelerated libraries translate to improved performance metrics, including:\n",
       "\n",
       "* **Speedup**: GPU-accelerated libraries can achieve speedups of 25x or more compared to CPU-based libraries for certain algorithms [Source 2].\n",
       "* **Throughput**: GPUs can process large datasets much faster than CPUs, leading to increased throughput for data-intensive tasks.\n",
       "* **Energy Efficiency**: GPUs can perform computations while consuming less power than CPUs, making them a more energy-efficient option for data science tasks.\n",
       "\n",
       "## Knowledge Gaps & Further Research\n",
       "\n",
       "* While the sources provide a solid foundation for understanding the advantages of GPU-accelerated libraries, there is a need for more research on optimizing GPU performance for specific data science tasks.\n",
       "* Further investigation is required to explore the limitations of GPU-accelerated libraries and identify scenarios where CPU-based libraries may be more suitable.\n",
       "* Additional research is needed to develop more efficient data transfer mechanisms between CPU and GPU memory.\n",
       "\n",
       "## Practical Usage Guidance\n",
       "\n",
       "When working with GPU-accelerated libraries, consider the following best practices:\n",
       "\n",
       "* Optimize data transfer between CPU and GPU memory to minimize overhead.\n",
       "* Leverage data parallelism and SIMD instructions to maximize GPU utilization.\n",
       "* Select algorithms that are well-suited for GPU acceleration, such as those with high computational complexity.\n",
       "* Monitor GPU memory usage and adjust batch sizes accordingly to avoid memory constraints.\n",
       "\n",
       "By understanding the architectural and computational advantages of modern GPUs, data scientists can harness the power of GPU-accelerated libraries to accelerate their data science tasks and achieve improved performance metrics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "final_response = answer_grader(question, graded_response, context, model, tokenizer)\n",
    "Markdown(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68558d-26a9-450d-a161-d521e0bd177b",
   "metadata": {},
   "source": [
    "## Complete Self-Corrective RAG System with Citations\n",
    "\n",
    "This cell implements the complete self-corrective RAG workflow using LangGraph. The system combines document retrieval, response generation, hallucination detection, answer quality assessment, and proper citation management in an orchestrated pipeline.\n",
    "\n",
    "**Core Components:**\n",
    "- **GraphState**: Comprehensive state management with chat history and document tracking\n",
    "- **Enhanced Retrieval**: Multi-source document retrieval with citation metadata\n",
    "- **Response Generation**: Context-aware generation with chat history integration\n",
    "- **Hallucination Detection**: Citation validation and content grounding assessment\n",
    "- **Answer Quality**: Strict grading for response adequacy\n",
    "- **Citation Management**: Automatic reference formatting for mixed source types\n",
    "- **Workflow Orchestration**: LangGraph-based pipeline with conditional routing and retry logic\n",
    "\n",
    "**Key Features:**\n",
    "- Document exclusion tracking to prevent repetitive retrievals\n",
    "- Progressive search expansion during regeneration attempts\n",
    "- Chat history management for follow-up question handling\n",
    "- Multi-source citation support (URLs, notebooks, Python files)\n",
    "- Comprehensive error handling and fallback mechanisms\n",
    "- Detailed workflow progress reporting and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "760a05f1-0034-4c25-8a57-da5878a25c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Enhanced graph state with chat history and document tracking\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    vector_store: object\n",
    "    model: object\n",
    "    tokenizer: object\n",
    "    context: str\n",
    "    response: str\n",
    "    is_grounded: bool\n",
    "    answers_question: bool\n",
    "    generation_count: int\n",
    "    max_retries: int\n",
    "    chat_history: List[Dict[str, str]]\n",
    "    retrieved_doc_ids: List[str]\n",
    "    source_metadata: List[Dict[str, str]]  # Track source metadata for citations\n",
    "\n",
    "# Utility function to get document ID\n",
    "def get_doc_id(doc):\n",
    "    \"\"\"Generate unique ID for a document\"\"\"\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    content_snippet = doc.page_content[:100].replace('\\n', ' ').strip()\n",
    "    return f\"{source}_{hash(content_snippet)}\"\n",
    "\n",
    "# Global variables for citation tracking\n",
    "_current_doc_ids = []\n",
    "_current_source_metadata = []\n",
    "\n",
    "# REPLACE YOUR EXISTING extract_source_metadata FUNCTION WITH THIS:\n",
    "\n",
    "def extract_source_metadata(documents):\n",
    "    \"\"\"\n",
    "    Extract citation-relevant metadata from mixed document sources\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        metadata = doc.metadata\n",
    "        source_type = metadata.get('source_type', 'unknown')\n",
    "        \n",
    "        if source_type == 'notebook':\n",
    "            # For notebooks, use folder/filename format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown notebook'),\n",
    "                \"title\": f\"Notebook: {metadata.get('filename', 'Unknown')}\",\n",
    "                \"source_type\": \"notebook\"\n",
    "            }\n",
    "        elif source_type == 'python_file':\n",
    "            # For Python files, use folder/filename format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown python file'),\n",
    "                \"title\": f\"Python File: {metadata.get('filename', 'Unknown')}\",\n",
    "                \"source_type\": \"python_file\"\n",
    "            }\n",
    "        else:\n",
    "            # For URLs, use existing format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown source'),\n",
    "                \"title\": metadata.get('title', 'Untitled'),\n",
    "                \"source_type\": \"url\"\n",
    "            }\n",
    "        \n",
    "        sources.append(source_info)\n",
    "    return sources\n",
    "\n",
    "def create_context_with_source_metadata(question: str, relevant_docs: List, model, tokenizer) -> tuple:\n",
    "    \"\"\"\n",
    "    Create context with source metadata for mixed sources (URLs + Notebooks + Python files)\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING CONTEXT WITH SOURCE METADATA---\")\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return f\"No relevant documents found for: {question}\", []\n",
    "    \n",
    "    # Extract source metadata for references\n",
    "    source_metadata = extract_source_metadata(relevant_docs)\n",
    "    \n",
    "    # Create knowledge base with FULL source information\n",
    "    knowledge_base = \"=== KNOWLEDGE BASE ===\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        content = doc.page_content.strip()\n",
    "        content = ' '.join(content.split())  # Clean whitespace\n",
    "        \n",
    "        # Get source info based on type\n",
    "        metadata = doc.metadata\n",
    "        source_type = metadata.get('source_type', 'unknown')\n",
    "        \n",
    "        if source_type == 'notebook':\n",
    "            source_display = f\"Notebook: {metadata.get('source', 'Unknown')}\"\n",
    "            source_title = f\"Notebook: {metadata.get('filename', 'Unknown')}\"\n",
    "        elif source_type == 'python_file':\n",
    "            source_display = f\"Python File: {metadata.get('source', 'Unknown')}\"\n",
    "            source_title = f\"Python File: {metadata.get('filename', 'Unknown')}\"\n",
    "        else:\n",
    "            source_display = metadata.get('source', 'Unknown source')\n",
    "            source_title = metadata.get('title', 'Untitled')\n",
    "        \n",
    "        knowledge_base += f\"### Source {i}: {source_title}\\n\"\n",
    "        knowledge_base += f\"Source: {source_display}\\n\"\n",
    "        knowledge_base += f\"Content: {content}\\n\\n\"\n",
    "    \n",
    "    # Generate refined query\n",
    "    refined_query = generate_refined_query(question, model, tokenizer)\n",
    "    \n",
    "    # Create source list for instructions\n",
    "    source_list = \"\"\n",
    "    for source in source_metadata:\n",
    "        if source['source_type'] == 'notebook':\n",
    "            source_list += f\"  [{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "        elif source['source_type'] == 'python_file':\n",
    "            source_list += f\"  [{source['number']}] {source['source']} (Python File)\\n\"\n",
    "        else:\n",
    "            source_list += f\"  [{source['number']}] {source['title']} - {source['source']}\\n\"\n",
    "    \n",
    "    # Create citation instructions with source information\n",
    "    citation_instructions = f\"\"\"Using the provided sources, generate a comprehensive response that:\n",
    "\n",
    "1. **Directly addresses the question**\n",
    "2. **Uses source material effectively with natural citations**\n",
    "3. **Maintains technical accuracy**\n",
    "\n",
    "CITATION APPROACH:\n",
    "- When referencing information from sources, use numbered citations [1], [2], [3]\n",
    "- You have access to these specific sources:\n",
    "\n",
    "{source_list}\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- Use [1], [2], [3] etc. when citing information from the sources above\n",
    "- Only reference the {len(relevant_docs)} sources provided ([1] through [{len(relevant_docs)}])\n",
    "- For notebook sources, citations will show as folder/filename in references\n",
    "- For Python file sources, citations will show as folder/filename in references\n",
    "- For URL sources, citations will show full URLs in references\n",
    "- Do NOT create fake URLs or additional sources\n",
    "- Focus on the content and cite naturally where information comes from these sources\n",
    "\n",
    "Generate your response using the source content and cite appropriately with [1], [2], etc.\"\"\"\n",
    "\n",
    "    # Assemble enhanced context\n",
    "    enhanced_context = f\"\"\"{knowledge_base}=== ENHANCED QUERY ===\n",
    "{refined_query}\n",
    "\n",
    "=== GENERATION INSTRUCTIONS ===\n",
    "{citation_instructions}\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "    return enhanced_context, source_metadata\n",
    "\n",
    "def format_final_response_with_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format the final response with proper references for mixed sources\n",
    "    \"\"\"\n",
    "    if not source_metadata:\n",
    "        return response\n",
    "    \n",
    "    # Check if LLM already created a references section\n",
    "    has_references = any(keyword in response.lower() for keyword in ['references:', 'bibliography:', 'sources:'])\n",
    "    \n",
    "    if has_references:\n",
    "        # LLM created references - validate them against our metadata\n",
    "        validated_response = validate_and_fix_references(response, source_metadata)\n",
    "        return validated_response\n",
    "    else:\n",
    "        # LLM didn't create references - add our clean section\n",
    "        references_section = \"\\n\\n**REFERENCES:**\\n\"\n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            elif source['source_type'] == 'python_file':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Python File)\\n\"\n",
    "            else:\n",
    "                references_section += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        return response + references_section\n",
    "\n",
    "def validate_and_fix_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Validate LLM-created references and fix any issues for mixed sources\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find reference sections\n",
    "    ref_pattern = r'(References?:|Bibliography:|Sources?:)(.*?)(?=\\n\\n[A-Z]|\\Z)'\n",
    "    \n",
    "    def fix_reference_section(match):\n",
    "        section_header = match.group(1)\n",
    "        \n",
    "        # Rebuild reference section with valid sources only\n",
    "        fixed_refs = f\"\\n{section_header}\\n\"\n",
    "        \n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            elif source['source_type'] == 'python_file':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Python File)\\n\"\n",
    "            else:\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        \n",
    "        return fixed_refs.rstrip()\n",
    "    \n",
    "    # Replace reference sections with validated ones\n",
    "    fixed_response = re.sub(ref_pattern, fix_reference_section, response, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    return fixed_response\n",
    "\n",
    "# REPLACE YOUR EXISTING format_final_response_with_references FUNCTION WITH THIS:\n",
    "\n",
    "def format_final_response_with_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format the final response with proper references for mixed sources (URLs + Notebooks)\n",
    "    \"\"\"\n",
    "    if not source_metadata:\n",
    "        return response\n",
    "    \n",
    "    # Check if LLM already created a references section\n",
    "    has_references = any(keyword in response.lower() for keyword in ['references:', 'bibliography:', 'sources:'])\n",
    "    \n",
    "    if has_references:\n",
    "        # LLM created references - validate them against our metadata\n",
    "        validated_response = validate_and_fix_references(response, source_metadata)\n",
    "        return validated_response\n",
    "    else:\n",
    "        # LLM didn't create references - add our clean section\n",
    "        references_section = \"\\n\\n**REFERENCES:**\\n\"\n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            else:\n",
    "                references_section += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        return response + references_section\n",
    "\n",
    "def validate_and_fix_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Validate LLM-created references and fix any issues for mixed sources\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find reference sections\n",
    "    ref_pattern = r'(References?:|Bibliography:|Sources?:)(.*?)(?=\\n\\n[A-Z]|\\Z)'\n",
    "    \n",
    "    def fix_reference_section(match):\n",
    "        section_header = match.group(1)\n",
    "        \n",
    "        # Rebuild reference section with valid sources only\n",
    "        fixed_refs = f\"\\n{section_header}\\n\"\n",
    "        \n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            else:\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        \n",
    "        return fixed_refs.rstrip()\n",
    "    \n",
    "    # Replace reference sections with validated ones\n",
    "    fixed_response = re.sub(ref_pattern, fix_reference_section, response, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    return fixed_response\n",
    "\n",
    "def validate_numbered_citations(response: str, num_sources: int) -> Dict:\n",
    "    \"\"\"Validate that all numbered citations [1], [2], etc. are valid\"\"\"\n",
    "    \n",
    "    # Find all citation patterns [1], [2], [1,2], [1, 2], etc.\n",
    "    citation_pattern = r'\\[([0-9,\\s]+)\\]'\n",
    "    citations = re.findall(citation_pattern, response)\n",
    "    \n",
    "    valid_numbers = set(range(1, num_sources + 1))\n",
    "    invalid_citations = []\n",
    "    valid_citations = []\n",
    "    \n",
    "    for citation in citations:\n",
    "        # Parse citation numbers\n",
    "        try:\n",
    "            cited_numbers = [int(x.strip()) for x in citation.split(',') if x.strip()]\n",
    "            for num in cited_numbers:\n",
    "                if num in valid_numbers:\n",
    "                    valid_citations.append(num)\n",
    "                else:\n",
    "                    invalid_citations.append(num)\n",
    "        except ValueError:\n",
    "            invalid_citations.append(citation)\n",
    "    \n",
    "    return {\n",
    "        \"has_invalid_citations\": len(invalid_citations) > 0,\n",
    "        \"invalid_citations\": invalid_citations,\n",
    "        \"valid_citations\": list(set(valid_citations)),\n",
    "        \"total_sources_available\": num_sources\n",
    "    }\n",
    "\n",
    "def generate_refined_query(question: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Generate refined query without regex issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Refine this technical question to be more specific and comprehensive:\n",
    "\n",
    "Original: {question}\n",
    "\n",
    "Create a refined version that asks for more detailed information. Respond with ONLY the refined question.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You refine technical questions to be more comprehensive.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.4\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        refined = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean without regex\n",
    "        refined = refined.strip(' \\'\"')\n",
    "        return refined if refined else question\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Query refinement failed: {e}\")\n",
    "        return question\n",
    "# Modified retrieve_node that uses source metadata\n",
    "def retrieve_node_with_citations(question: str, vector_store, model, tokenizer, max_attempts: int = 3,\n",
    "                                k=5, exclude_ids=None, attempt_number=0, return_doc_ids=False):\n",
    "    \"\"\"Enhanced retrieve node with citation tracking\"\"\"\n",
    "    global _current_doc_ids, _current_source_metadata\n",
    "    \n",
    "    print(f\"ENHANCED RETRIEVE NODE: {question}\")\n",
    "    print(\"Target: At least 3 relevant + drop not_relevant documents\")\n",
    "    \n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f\"\\n--- Retrieval Attempt {attempt} ---\")\n",
    "        \n",
    "        # Step 1: Retrieve documents with exclusion logic\n",
    "        try:\n",
    "            if exclude_ids:\n",
    "                print(f\"Excluding {len(exclude_ids)} previously retrieved documents\")\n",
    "                all_documents = vector_store.similarity_search(question, k=k*3)\n",
    "                filtered_docs = []\n",
    "                excluded_count = 0\n",
    "                \n",
    "                for doc in all_documents:\n",
    "                    doc_id = get_doc_id(doc)\n",
    "                    if doc_id not in exclude_ids:\n",
    "                        filtered_docs.append(doc)\n",
    "                        if len(filtered_docs) >= k:\n",
    "                            break\n",
    "                    else:\n",
    "                        excluded_count += 1\n",
    "                \n",
    "                documents = filtered_docs\n",
    "                print(f\"Excluded {excluded_count} documents, using {len(documents)} new documents\")\n",
    "            else:\n",
    "                documents = vector_store.similarity_search(question, k=k)\n",
    "                \n",
    "            print(f\"‚úì Retrieved {len(documents)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Retrieval failed: {e}\")\n",
    "            if attempt >= max_attempts:\n",
    "                return f\"Unable to retrieve documents for: {question}\", []\n",
    "            continue\n",
    "        \n",
    "        if len(documents) < 3:\n",
    "            print(f\"‚ö†Ô∏è  Only {len(documents)} documents found\")\n",
    "            if attempt >= max_attempts:\n",
    "                if documents:\n",
    "                    break\n",
    "                else:\n",
    "                    return f\"No documents found for: {question}\", []\n",
    "        \n",
    "        # Step 2: Grade documents (using your existing function)\n",
    "        grades = grade_documents_strict(question, documents, model, tokenizer)\n",
    "        \n",
    "        # Step 3: Filter out irrelevant documents (using your existing function)\n",
    "        relevant_docs = filter_and_show_documents(documents, grades)\n",
    "        dropped_count = len(documents) - len(relevant_docs)\n",
    "        print(f\"  Immediately filtered: Kept {len(relevant_docs)}, dropped {dropped_count}\")\n",
    "        \n",
    "        # Step 4: Analyze the FILTERED results\n",
    "        highly_relevant_count = count_by_relevance(grades, \"highly_relevant\")\n",
    "        somewhat_relevant_count = count_by_relevance(grades, \"somewhat_relevant\")\n",
    "        \n",
    "        print(f\"Grading Analysis (after filtering):\")\n",
    "        print(f\"  Highly relevant: {highly_relevant_count}\")\n",
    "        print(f\"  Somewhat relevant: {somewhat_relevant_count}\") \n",
    "        print(f\"  Total relevant kept: {len(relevant_docs)}\")\n",
    "        \n",
    "        # Step 5: Check quality requirements\n",
    "        if highly_relevant_count >= 1 and len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì SUCCESS: Found {highly_relevant_count} highly relevant + {len(relevant_docs)} total relevant\")\n",
    "            \n",
    "            # Generate enhanced context with source metadata\n",
    "            enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs and source metadata\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            _current_doc_ids = used_doc_ids\n",
    "            _current_source_metadata = source_metadata\n",
    "            \n",
    "            print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids, source_metadata\n",
    "            else:\n",
    "                return enhanced_context, source_metadata\n",
    "        \n",
    "        elif len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì ACCEPTABLE: Found {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            _current_doc_ids = used_doc_ids\n",
    "            _current_source_metadata = source_metadata\n",
    "            \n",
    "            print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids, source_metadata\n",
    "            else:\n",
    "                return enhanced_context, source_metadata\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚úó INSUFFICIENT: Only {len(relevant_docs)} relevant documents after filtering\")\n",
    "            \n",
    "            if attempt >= max_attempts:\n",
    "                print(\"Max attempts reached\")\n",
    "                if relevant_docs:\n",
    "                    enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "                    \n",
    "                    used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "                    _current_doc_ids = used_doc_ids\n",
    "                    _current_source_metadata = source_metadata\n",
    "                    \n",
    "                    print(f\"‚ö†Ô∏è  Using {len(relevant_docs)} available relevant documents\")\n",
    "                    \n",
    "                    if return_doc_ids:\n",
    "                        return enhanced_context, used_doc_ids, source_metadata\n",
    "                    else:\n",
    "                        return enhanced_context, source_metadata\n",
    "                else:\n",
    "                    if return_doc_ids:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", [], []\n",
    "                    else:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", []\n",
    "            else:\n",
    "                print(f\"Will search for more documents in next attempt...\")\n",
    "                k = min(k + 3, 15)\n",
    "    \n",
    "    if return_doc_ids:\n",
    "        return f\"Failed to find adequate documents for: {question}\", [], []\n",
    "    else:\n",
    "        return f\"Failed to find adequate documents for: {question}\", []\n",
    "\n",
    "# Modified retrieve wrapper for citations\n",
    "def retrieve_node_wrapper_with_citations(state):\n",
    "    \"\"\"Wrapper with citation tracking\"\"\"\n",
    "    print(\"---RETRIEVE NODE---\")\n",
    "    \n",
    "    generation_count = state.get(\"generation_count\", 0)\n",
    "    retrieved_doc_ids = state.get(\"retrieved_doc_ids\", [])\n",
    "    \n",
    "    # Determine k based on attempt number\n",
    "    if generation_count == 0:\n",
    "        k = 5\n",
    "        print(\"üÜï First retrieval attempt\")\n",
    "    elif generation_count == 1:\n",
    "        k = 8\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - expanding search to k={k}\")\n",
    "    elif generation_count == 2:\n",
    "        k = 12\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - expanding search to k={k}\")\n",
    "    else:\n",
    "        k = 15\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - maximum search k={k}\")\n",
    "    \n",
    "    # Call enhanced retrieve_node with citations\n",
    "    context, source_metadata = retrieve_node_with_citations(\n",
    "        question=state[\"question\"],\n",
    "        vector_store=state[\"vector_store\"],\n",
    "        model=state[\"model\"],\n",
    "        tokenizer=state[\"tokenizer\"],\n",
    "        max_attempts=3,\n",
    "        k=k,\n",
    "        exclude_ids=retrieved_doc_ids,\n",
    "        attempt_number=generation_count\n",
    "    )\n",
    "    \n",
    "    # Update retrieved document IDs and source metadata\n",
    "    global _current_doc_ids\n",
    "    updated_doc_ids = retrieved_doc_ids + _current_doc_ids\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"retrieved_doc_ids\": updated_doc_ids,\n",
    "        \"source_metadata\": source_metadata\n",
    "    }\n",
    "\n",
    "def build_enhanced_chat_context(state):\n",
    "    \"\"\"Build enhanced chat context that helps with follow-ups\"\"\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    current_question = state[\"question\"]\n",
    "    \n",
    "    if not chat_history:\n",
    "        return \"\"\n",
    "    \n",
    "    # Check if current question is a follow-up\n",
    "    followup_indicators = [\n",
    "        \"example\", \"code\", \"show me\", \"demonstrate\", \"implement\", \"using\",\n",
    "        \"with\", \"how to\", \"can you\", \"provide\", \"give me\", \"write\",\n",
    "        \"this\", \"that\", \"it\", \"above\", \"previous\", \"mentioned\"\n",
    "    ]\n",
    "    \n",
    "    is_followup = any(indicator in current_question.lower() for indicator in followup_indicators)\n",
    "    \n",
    "    if is_followup and chat_history:\n",
    "        # For follow-ups, provide rich context\n",
    "        last_exchange = chat_history[-1]\n",
    "        \n",
    "        chat_context = f\"\"\"\n",
    "=== CONVERSATION CONTEXT ===\n",
    "This is a follow-up question building on our previous discussion.\n",
    "\n",
    "PREVIOUS TOPIC: {last_exchange['question']}\n",
    "\n",
    "PREVIOUS DISCUSSION SUMMARY: \n",
    "{last_exchange['response'][:800]}...\n",
    "\n",
    "CURRENT FOLLOW-UP: {current_question}\n",
    "\n",
    "INSTRUCTION: The current question \"{current_question}\" is asking you to build upon, extend, or provide examples related to the previous topic \"{last_exchange['question']}\". Use the retrieved sources to provide specific examples, code, or implementations that relate to what we just discussed.\n",
    "\"\"\"\n",
    "    else:\n",
    "        # For new topics, provide minimal context\n",
    "        chat_context = f\"\"\"\n",
    "=== CONVERSATION CONTEXT ===\n",
    "Previous conversation exists but this appears to be a new topic.\n",
    "\n",
    "Recent discussion: {chat_history[-1]['question'] if chat_history else 'None'}\n",
    "\n",
    "CURRENT QUESTION: {current_question}\n",
    "\"\"\"\n",
    "    \n",
    "    return chat_context\n",
    "\n",
    "def generate_node_wrapper_with_citations(state):\n",
    "    \"\"\"Wrapper for your existing generate_response function\"\"\"\n",
    "    print(\"---GENERATE NODE---\")\n",
    "    \n",
    "    # Build ENHANCED chat history context\n",
    "    chat_context = build_enhanced_chat_context(state)\n",
    "    \n",
    "    if chat_context:\n",
    "        print(f\"Using enhanced chat context\")\n",
    "    \n",
    "    # Call YOUR existing generate_response function\n",
    "    try:\n",
    "        response, _ = generate_response(\n",
    "            question=state[\"question\"],\n",
    "            vector_store=state[\"vector_store\"],\n",
    "            model=state[\"model\"],\n",
    "            tokenizer=state[\"tokenizer\"],\n",
    "            max_tokens=2000,\n",
    "            temperature=0.3,\n",
    "            context=state[\"context\"],\n",
    "            chat_context=chat_context\n",
    "        )\n",
    "        \n",
    "        generation_count = state.get(\"generation_count\", 0) + 1\n",
    "        print(f\"Generated {len(response)} characters\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"generation_count\": generation_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {e}\")\n",
    "        return {\n",
    "            \"response\": f\"Generation error: {e}\",\n",
    "            \"generation_count\": state.get(\"generation_count\", 0) + 1\n",
    "        }\n",
    "\n",
    "def hallucination_grade_wrapper_with_citations(state):\n",
    "    \"\"\"Enhanced hallucination grader with citation validation\"\"\"\n",
    "    print(\"---HALLUCINATION GRADER WITH CITATION VALIDATION---\")\n",
    "    \n",
    "    context = state[\"context\"]\n",
    "    response = state[\"response\"]\n",
    "    source_metadata = state.get(\"source_metadata\", [])\n",
    "    \n",
    "    # Step 1: Validate numbered citations\n",
    "    citation_check = validate_numbered_citations(response, len(source_metadata))\n",
    "    \n",
    "    if citation_check[\"has_invalid_citations\"]:\n",
    "        print(f\"‚úó Invalid citations found: {citation_check['invalid_citations']}\")\n",
    "        print(f\"  Available citation numbers: 1-{citation_check['total_sources_available']}\")\n",
    "        return {\"is_grounded\": False}\n",
    "    else:\n",
    "        print(f\"‚úì All citations valid: {citation_check['valid_citations']}\")\n",
    "    \n",
    "    # Step 2: Regular hallucination check\n",
    "    if \"=== KNOWLEDGE BASE ===\" in context:\n",
    "        kb_start = context.find(\"=== KNOWLEDGE BASE ===\") + len(\"=== KNOWLEDGE BASE ===\")\n",
    "        kb_end = context.find(\"=== ENHANCED QUERY ===\")\n",
    "        if kb_end == -1:\n",
    "            knowledge_base = context[kb_start:].strip()\n",
    "        else:\n",
    "            knowledge_base = context[kb_start:kb_end].strip()\n",
    "    else:\n",
    "        knowledge_base = context\n",
    "    \n",
    "    check_prompt = f\"\"\"KNOWLEDGE BASE:\n",
    "{knowledge_base}\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "Is this response grounded in the knowledge base? Does it only use information from the provided sources? Are all numbered citations [1], [2], etc. valid?\n",
    "\n",
    "Answer only: GROUNDED or HALLUCINATED\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "    \n",
    "    try:\n",
    "        input_ids = state[\"tokenizer\"].apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(state[\"model\"].device)\n",
    "        \n",
    "        terminators = [\n",
    "            state[\"tokenizer\"].eos_token_id,\n",
    "            state[\"tokenizer\"].convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = state[\"model\"].generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=state[\"tokenizer\"].eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = state[\"tokenizer\"].decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        is_grounded = \"GROUNDED\" in grade\n",
    "        print(f\"Content grounded: {is_grounded}\")\n",
    "        \n",
    "        return {\"is_grounded\": is_grounded}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Hallucination check failed: {e}\")\n",
    "        return {\"is_grounded\": True}\n",
    "\n",
    "def answer_grade_wrapper_with_citations(state):\n",
    "    \"\"\"Check if response answers the question\"\"\"\n",
    "    print(\"---ANSWER GRADER---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    response = state[\"response\"]\n",
    "    \n",
    "    check_prompt = f\"\"\"QUESTION: {question}\n",
    "\n",
    "RESPONSE: {response}\n",
    "\n",
    "Does this response directly and adequately answer the specific question that was asked?\n",
    "\n",
    "Answer only: ANSWERS or DOESNT_ANSWER\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "    \n",
    "    try:\n",
    "        input_ids = state[\"tokenizer\"].apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(state[\"model\"].device)\n",
    "        \n",
    "        terminators = [\n",
    "            state[\"tokenizer\"].eos_token_id,\n",
    "            state[\"tokenizer\"].convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = state[\"model\"].generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=state[\"tokenizer\"].eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = state[\"tokenizer\"].decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        answers_question = \"ANSWERS\" in grade\n",
    "        print(f\"Answers question: {answers_question}\")\n",
    "        \n",
    "        # Update chat history when we have a successful response\n",
    "        updated_history = state.get(\"chat_history\", []).copy()\n",
    "        if answers_question:\n",
    "            updated_history.append({\n",
    "                \"question\": state[\"question\"],\n",
    "                \"response\": state[\"response\"]\n",
    "            })\n",
    "            # Keep only last 5 exchanges to prevent memory bloat\n",
    "            if len(updated_history) > 5:\n",
    "                updated_history = updated_history[-5:]\n",
    "        \n",
    "        return {\n",
    "            \"answers_question\": answers_question,\n",
    "            \"chat_history\": updated_history\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer check failed: {e}\")\n",
    "        return {\"answers_question\": True}\n",
    "\n",
    "def final_node_wrapper_with_citations(state):\n",
    "    \"\"\"Handle max retries reached\"\"\"\n",
    "    print(\"---MAX RETRIES REACHED---\")\n",
    "    response = f\"‚ö†Ô∏è Warning: Max retries reached.\\n\\n{state['response']}\"\n",
    "    \n",
    "    # Still update chat history even for failed attempts\n",
    "    updated_history = state.get(\"chat_history\", []).copy()\n",
    "    updated_history.append({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"response\": response\n",
    "    })\n",
    "    if len(updated_history) > 5:\n",
    "        updated_history = updated_history[-5:]\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"chat_history\": updated_history\n",
    "    }\n",
    "\n",
    "# Conditional edge functions\n",
    "def decide_after_hallucination(state):\n",
    "    \"\"\"Route after hallucination grading\"\"\"\n",
    "    if state[\"is_grounded\"]:\n",
    "        return \"check_answer\"\n",
    "    elif state[\"generation_count\"] >= state[\"max_retries\"]:\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        return \"regenerate\"\n",
    "\n",
    "def decide_after_answer(state):\n",
    "    \"\"\"Route after answer grading\"\"\"\n",
    "    if state[\"answers_question\"]:\n",
    "        return \"end\"\n",
    "    elif state[\"generation_count\"] >= state[\"max_retries\"]:\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        return \"regenerate\"\n",
    "\n",
    "# Global chat history storage\n",
    "_conversation_history = []\n",
    "\n",
    "# Main GPU tutor function with citations\n",
    "def gpu_tutor(question = \"\", vector_store = None, model = None, tokenizer = None, max_retries=3, reset_history=False, visualize = False):\n",
    "    \"\"\"\n",
    "    Complete GPU tutor workflow with numbered citations - returns only the final response string\n",
    "    \"\"\"\n",
    "    global _conversation_history\n",
    "    \n",
    "    # Reset history if requested\n",
    "    if reset_history:\n",
    "        _conversation_history = []\n",
    "        print(\"üîÑ Chat history reset\")\n",
    "    \n",
    "    print(f\"GPU TUTOR WORKFLOW: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create workflow\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes with citation support\n",
    "    workflow.add_node(\"retrieve\", retrieve_node_wrapper_with_citations)\n",
    "    workflow.add_node(\"generate\", generate_node_wrapper_with_citations)\n",
    "    workflow.add_node(\"grade_hallucination\", hallucination_grade_wrapper_with_citations)\n",
    "    workflow.add_node(\"grade_answer\", answer_grade_wrapper_with_citations)\n",
    "    workflow.add_node(\"final\", final_node_wrapper_with_citations)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"grade_hallucination\")\n",
    "    \n",
    "    # Conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_hallucination\",\n",
    "        decide_after_hallucination,\n",
    "        {\n",
    "            \"check_answer\": \"grade_answer\",\n",
    "            \"regenerate\": \"retrieve\",  # Go back to retrieve for NEW documents\n",
    "            \"finish\": \"final\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_answer\",\n",
    "        decide_after_answer,\n",
    "        {\n",
    "            \"end\": END,\n",
    "            \"regenerate\": \"retrieve\",  # Go back to retrieve for NEW documents\n",
    "            \"finish\": \"final\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"final\", END)\n",
    "    \n",
    "    # Compile and run\n",
    "    app = workflow.compile()\n",
    "    if visualize:\n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        # Get PNG binary data\n",
    "        png_data = app.get_graph().draw_mermaid_png()\n",
    "    \n",
    "        # Save the PNG to a file\n",
    "        with open(\"workflow_visualization.png\", \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "    \n",
    "        # Display the image in Jupyter Notebook\n",
    "        display(Image(png_data))\n",
    "        return 0\n",
    "    \n",
    "    # Initial state - FRESH document tracking for each new question\n",
    "    initial_state = {\n",
    "        \"question\": question,\n",
    "        \"vector_store\": vector_store,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"context\": \"\",\n",
    "        \"response\": \"\",\n",
    "        \"is_grounded\": False,\n",
    "        \"answers_question\": False,\n",
    "        \"generation_count\": 0,\n",
    "        \"max_retries\": max_retries,\n",
    "        \"chat_history\": _conversation_history.copy(),\n",
    "        \"retrieved_doc_ids\": [],  # FRESH for each question\n",
    "        \"source_metadata\": []     # FRESH for each question\n",
    "    }\n",
    "    \n",
    "    # Run workflow\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    # Update global chat history (document tracking cleared for next question)\n",
    "    _conversation_history = final_state.get(\"chat_history\", [])\n",
    "    \n",
    "    # Format final response with numbered references\n",
    "    final_response = format_final_response_with_references(\n",
    "        final_state[\"response\"], \n",
    "        final_state.get(\"source_metadata\", [])\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WORKFLOW COMPLETE\")\n",
    "    print(f\"Total generation attempts: {final_state['generation_count']}\")\n",
    "    print(f\"Final response grounded: {final_state.get('is_grounded', 'Unknown')}\")\n",
    "    print(f\"Final response answers question: {final_state.get('answers_question', 'Unknown')}\")\n",
    "    print(f\"Chat history length: {len(_conversation_history)}\")\n",
    "    print(f\"Documents used this question: {len(final_state.get('retrieved_doc_ids', []))}\")\n",
    "    print(f\"Sources cited: {len(final_state.get('source_metadata', []))}\")\n",
    "    \n",
    "    # Return ONLY the response string with references\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec0114-82f7-4bce-a85c-970302e52f64",
   "metadata": {},
   "source": [
    "## Demonstration of GPU ChatBot Interface\n",
    "\n",
    "**ReadMe**\n",
    "- Add your question to 'question' as a string (\"\")\n",
    "- If talking for the first time, keep 'reset_history' as True\n",
    "- If following up, keep 'reset_history' as False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a56203d-6de0-409c-ae04-7b5981cce315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Chat history reset\n",
      "GPU TUTOR WORKFLOW: What is memory optimization in machine learning?\n",
      "================================================================================\n",
      "---RETRIEVE NODE---\n",
      "üÜï First retrieval attempt\n",
      "ENHANCED RETRIEVE NODE: What is memory optimization in machine learning?\n",
      "Target: At least 3 relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"directly addresses memory optimization in machine learning\"}, {\"doc_i...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úó Dropping Doc 2: not_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 3\n",
      "  Somewhat relevant: 1\n",
      "  Total relevant kept: 4\n",
      "‚úì SUCCESS: Found 3 highly relevant + 4 total relevant\n",
      "---GENERATING CONTEXT WITH SOURCE METADATA---\n",
      "‚úì Enhanced context with source metadata generated for 4 sources\n",
      "‚úì Enhanced context with source metadata generated for 4 sources\n",
      "---GENERATE NODE---\n",
      "GPU TUTOR: What is memory optimization in machine learning?\n",
      "================================================================================\n",
      "Step 1: Using provided context...\n",
      "Step 2: Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated response (5110 characters)\n",
      "================================================================================\n",
      "Generated 5110 characters\n",
      "---HALLUCINATION GRADER WITH CITATION VALIDATION---\n",
      "‚úì All citations valid: [1, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content grounded: False\n",
      "---RETRIEVE NODE---\n",
      "üîÑ Regeneration attempt #1 - expanding search to k=8\n",
      "ENHANCED RETRIEVE NODE: What is memory optimization in machine learning?\n",
      "Target: At least 3 relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "Excluding 4 previously retrieved documents\n",
      "Excluded 4 documents, using 8 new documents\n",
      "‚úì Retrieved 8 documents\n",
      "---DOCUMENT GRADING (8 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"not_relevant\", \"reason\": \"unrelated to memory optimization in machine learning\"}, {\"doc_index\": 2,...\n",
      "  Document filtering:\n",
      "    ‚úó Dropping Doc 1: not_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "    ‚úó Dropping Doc 6: not_relevant\n",
      "    ‚úó Dropping Doc 7: not_relevant\n",
      "    ‚úó Dropping Doc 8: not_relevant\n",
      "  Result: Kept 4 relevant, dropped 4 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 4\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING CONTEXT WITH SOURCE METADATA---\n",
      "‚úì Enhanced context with source metadata generated for 4 sources\n",
      "‚úì Enhanced context with source metadata generated for 4 sources\n",
      "---GENERATE NODE---\n",
      "GPU TUTOR: What is memory optimization in machine learning?\n",
      "================================================================================\n",
      "Step 1: Using provided context...\n",
      "Step 2: Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated response (4135 characters)\n",
      "================================================================================\n",
      "Generated 4135 characters\n",
      "---HALLUCINATION GRADER WITH CITATION VALIDATION---\n",
      "‚úì All citations valid: [1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content grounded: True\n",
      "---ANSWER GRADER---\n",
      "Answers question: True\n",
      "================================================================================\n",
      "WORKFLOW COMPLETE\n",
      "Total generation attempts: 2\n",
      "Final response grounded: True\n",
      "Final response answers question: True\n",
      "Chat history length: 1\n",
      "Documents used this question: 8\n",
      "Sources cited: 4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Memory Optimization in Machine Learning: Techniques and Strategies**\n",
       "\n",
       "Memory optimization is a critical aspect of machine learning, particularly when dealing with large datasets and complex models. The goal of memory optimization is to minimize memory usage while maintaining model performance, thereby improving training speed, inference latency, and overall scalability.\n",
       "\n",
       "**Batching and Data Augmentation**\n",
       "\n",
       "One effective technique for memory optimization is batching, which involves processing multiple inputs simultaneously [1]. This approach reduces the time spent transferring data between the CPU and GPU, leading to significant speedups in training time and reduced GPU memory usage. Additionally, data augmentation can be used to generate new training data by applying random transformations to existing data, which can improve model generalization and reduce the need for new data [1].\n",
       "\n",
       "**Mixed Precision Training**\n",
       "\n",
       "Another strategy for memory optimization is mixed precision training, which involves using both single-precision and half-precision floating-point data types [1]. By leveraging half-precision data types, memory usage can be reduced, resulting in faster training times.\n",
       "\n",
       "**GPU Profiling and Memory Management**\n",
       "\n",
       "Monitoring GPU usage and memory allocation is essential for optimizing memory efficiency. TensorFlow provides profiling tools to identify performance bottlenecks and optimize code for maximum efficiency [1]. Similarly, PyTorch's memory allocator can be used to share GPU memory effectively between RAPIDS and PyTorch, reducing the risk of out-of-memory errors [2].\n",
       "\n",
       "**Fusing Optimizer Step into Backward Pass**\n",
       "\n",
       "A novel technique for saving memory is to fuse the optimizer step into the backward pass, which can reduce memory usage during training [3]. This approach involves capturing and visualizing memory snapshots to identify opportunities for optimization.\n",
       "\n",
       "**RAPIDS Memory Manager Pool**\n",
       "\n",
       "The RAPIDS Memory Manager (RMM) is a powerful tool for managing memory on the GPU throughout an end-to-end workflow [4]. By creating and sharing a single pool of memory, RMM can perform a single, large allocation of memory upfront, avoiding the latency of per-computation memory allocations mid-workflow. This approach can result in significant speedups, as demonstrated by a 4.6x speedup in a single GPU example [4].\n",
       "\n",
       "**Impact on Performance and Scalability**\n",
       "\n",
       "These memory optimization techniques and strategies can have a profound impact on the performance and scalability of deep learning models. By reducing memory usage and minimizing data transfer between the CPU and GPU, training speeds can be significantly improved. Additionally, inference latency can be reduced, enabling faster deployment of models in production environments. Furthermore, model accuracy can be maintained or even improved through the use of techniques like data augmentation and mixed precision training.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "In conclusion, memory optimization is a critical component of machine learning, and various techniques and strategies can be employed to minimize memory usage while maintaining model performance. By leveraging batching, data augmentation, mixed precision training, GPU profiling, and memory management tools like RAPIDS Memory Manager Pool, developers can improve the performance and scalability of their deep learning models.\n",
       "\n",
       "**Knowledge Gaps & Further Research**\n",
       "\n",
       "* Investigating the application of compression algorithms to reduce memory usage\n",
       "* Exploring the use of caching mechanisms to reduce data transfer between the CPU and GPU\n",
       "* Developing more advanced model pruning techniques to reduce memory usage without compromising model accuracy\n",
       "* Investigating the impact of memory optimization on edge AI and IoT applications\n",
       "\n",
       "**Practical Usage Guidance**\n",
       "\n",
       "When implementing memory optimization techniques, it is essential to carefully evaluate the trade-offs between memory usage, training speed, and model accuracy. Developers should experiment with different approaches and monitor memory usage and performance metrics to determine the most effective strategy for their specific use case.\n",
       "\n",
       "**REFERENCES:**\n",
       "[1] https://acecloud.ai/resources/blog/tensorflow-gpu/\n",
       "[2] https://medium.com/rapids-ai/pytorch-rapids-rmm-maximize-the-memory-efficiency-of-your-workflows-f475107ba4d4?source=collection_home---4------16-----------------------\n",
       "[3] https://docs.pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html\n",
       "[4] https://medium.com/rapids-ai/rapids-memory-manager-pool-speed-up-your-memory-allocations-3bc53929066a?source=collection_home---4------19-----------------------\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is memory optimization in machine learning?\"\n",
    "response = gpu_tutor(question, vector_store, model, tokenizer, reset_history = True)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c955e-0c26-44e0-87a1-9fccc767986d",
   "metadata": {},
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b3c0ac-cba4-498a-ae64-e602c9640918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU TUTOR WORKFLOW: \n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tutor(visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
